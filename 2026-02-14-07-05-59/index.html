<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI/CV Â≠¶ÊúØËÆ∫Êñá</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
        }
        
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px 20px;
            text-align: center;
            border-radius: 8px;
            margin-bottom: 30px;
        }
        
        header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }
        
        header p {
            font-size: 1.1em;
            opacity: 0.9;
        }
        
        .meta {
            margin-top: 15px;
            font-size: 0.9em;
            opacity: 0.8;
        }
        
        .config-info {
            background: white;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        .config-info h3 {
            color: #333;
            margin-bottom: 15px;
            font-size: 1.1em;
        }
        
        .config-info .source-list {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin-bottom: 15px;
        }
        
        .config-info .source-tag {
            background: #667eea;
            color: white;
            padding: 4px 12px;
            border-radius: 15px;
            font-size: 0.85em;
        }
        
        .config-info .filter-list {
            color: #666;
            font-size: 0.9em;
        }
        
        .config-info .filter-list strong {
            color: #333;
        }
        
        .config-info .source-stats {
            margin-top: 10px;
            padding-top: 10px;
            border-top: 1px solid #eee;
        }
        
        .config-info .source-stats strong {
            color: #333;
        }
        
        .config-info .stat-tag {
            background: #e3f2fd;
            color: #1565c0;
            padding: 3px 10px;
            border-radius: 12px;
            font-size: 0.85em;
            margin-right: 8px;
            display: inline-block;
            margin-top: 5px;
        }
        
        .items {
            display: grid;
            gap: 20px;
        }
        
        .item {
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            transition: transform 0.2s, box-shadow 0.2s;
        }
        
        .item:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
        }
        
        .item h2 {
            font-size: 1.4em;
            color: #667eea;
            margin-bottom: 10px;
        }
        
        .item .authors {
            color: #666;
            font-size: 0.9em;
            margin-bottom: 10px;
        }
        
        .item .content {
            color: #555;
            margin: 10px 0;
        }
        
        .item .footer {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-top: 15px;
            padding-top: 15px;
            border-top: 1px solid #eee;
            font-size: 0.85em;
            color: #888;
            flex-wrap: wrap;
            gap: 10px;
        }
        
        .item .source-info {
            display: flex;
            gap: 8px;
            align-items: center;
            flex-wrap: wrap;
        }
        
        .item .source-type {
            background: #667eea;
            color: white;
            padding: 3px 10px;
            border-radius: 12px;
            font-size: 0.8em;
        }
        
        .item .venue {
            background: #e8f5e9;
            color: #2e7d32;
            padding: 3px 10px;
            border-radius: 12px;
            font-size: 0.8em;
        }
        
        .item .categories {
            display: flex;
            gap: 5px;
            flex-wrap: wrap;
        }
        
        .item .category {
            background: #e3f2fd;
            color: #1565c0;
            padding: 2px 8px;
            border-radius: 4px;
            font-size: 0.8em;
        }
        
        .item .date {
            color: #666;
        }
        
        .item a {
            color: #667eea;
            text-decoration: none;
        }
        
        .item a:hover {
            text-decoration: underline;
        }
        
        .btn-pdf {
            display: inline-block;
            background: #d32f2f;
            color: white;
            padding: 5px 12px;
            border-radius: 4px;
            font-size: 0.85em;
        }
        
        .btn-pdf:hover {
            background: #b71c1c;
            text-decoration: none;
        }
        
        footer {
            text-align: center;
            padding: 30px;
            color: #888;
            font-size: 0.9em;
        }
        
        .empty {
            text-align: center;
            padding: 60px 20px;
            color: #888;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>AI/CV Â≠¶ÊúØËÆ∫Êñá</h1>
            <p>Ëá™Âä®ËøΩË∏™ CVPR, NeurIPS, AAAI, ICLR Á≠âÈ°∂‰ºöÊúÄÊñ∞ËÆ∫Êñá</p>
            <div class="meta">
                ÁîüÊàêÊó∂Èó¥: 2026-02-14 07:05:59
            </div>
        </header>
        
        
        <div class="config-info">
            <h3>üìã ÈÖçÁΩÆ‰ø°ÊÅØ</h3>
            <div class="source-list">
                <strong>Êï∞ÊçÆÊ∫ê:</strong>
                
                    
                    
                        
                        <span class="source-tag">DBLP</span>
                        
                        
                    
                        
                        
                    
                        
                        <span class="source-tag">arXiv</span>
                        
                        
                    
                        
                        
                    
                
            </div>
            
            
            
            <div class="source-stats">
                <strong>üìä Êù•Ê∫êÁªüËÆ°:</strong>
                
                <span class="stat-tag">DBLP: 13 ÁØá</span>
                
                <span class="stat-tag">arXiv: 12 ÁØá</span>
                
            </div>
            
            
            <div class="filter-list">
                <strong>ÂÖ≥ÈîÆËØçËøáÊª§:</strong>
                
                    
                        
                            VLA, vision, robotics, embodied, manipulation, detection, segmentation, learning, neural, deep, LLM, multimodal
                        
                    
                        
                    
                        
                    
                
            </div>
        </div>
        
        <div class="items">
            
                
                
                <div class="item">
                    <h2>Forty-second International Conference on Machine Learning, ICML 2025, Vancouver, BC, Canada, July 13...</h2>
                    
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ICML</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ICML 
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ICML</span>
                            
                        </div>
                        <span class="date"></span>
                        
                            
                            <a href="https://dblp.org/rec/conf/icml/2025p" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Position: Not All Explanations for Deep Learning Phenomena Are Equally Valuable.</h2>
                    
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ICML</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ICML 
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ICML</span>
                            
                        </div>
                        <span class="date"></span>
                        
                            
                            <a href="https://dblp.org/rec/conf/icml/JeffaresS25" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Position: Principles of Animal Cognition to Improve LLM Evaluations.</h2>
                    
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ICML</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ICML 
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ICML</span>
                            
                        </div>
                        <span class="date"></span>
                        
                            
                            <a href="https://dblp.org/rec/conf/icml/RaneKTRLCF25" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Dream-VL &amp; Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Jiacheng Ye, Shansan Gong, Jiahui Gao, Junming Fan, Shuang Wu, Wei Bi, Haoli Bai, Lifeng Shang, Lingpeng Kong
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">CoRR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        CoRR (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://arxiv.org/abs/2512.22615" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>VLM4VLA: Revisiting Vision-Language-Models in Vision-Language-Action Models.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Jianke Zhang, Xiaoyu Chen, Qiuyue Wang, Mingsheng Li, Yanjiang Guo, Yucheng Hu, Jiajun Zhang, Shuai Bai, Junyang Lin, Jianyu Chen
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">CoRR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        CoRR (2026)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                        </div>
                        <span class="date">2026</span>
                        
                            
                            <a href="https://arxiv.org/abs/2601.03309" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Contin...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Hongyu Ding, Ziming Xu, Yudong Fang, You Wu, Zixuan Chen, Jieqi Shi, Jing Huo, Yifan Zhang 0004, Yang Gao 0001
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">CoRR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        CoRR (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://arxiv.org/abs/2510.19655" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>MAPS: Preserving Vision-Language Representations via Module-Wise Proximity Scheduling for Better Vis...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Chengyue Huang, Mellon M. Zhang, Robert Azarcon, Glen Chou, Zsolt Kira
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">CoRR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        CoRR (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://arxiv.org/abs/2511.19878" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>V2A - Vision to Action: Learning Robotic Arm Actions Based on Vision and Language.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Michal Nazarczuk, Krystian Mikolajczyk
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ACCV</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ACCV (2020)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ACCV</span>
                            
                        </div>
                        <span class="date">2020</span>
                        
                            
                            <a href="https://doi.org/10.1007/978-3-030-69535-4_44" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Parameter-efficient action planning with large language models for vision-and-language navigation.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Bahram Mohammadi, Ehsan Abbasnejad, Yuankai Qi, Qi Wu 0001, Anton van den Hengel, Javen Qinfeng Shi
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">Pattern Recognit.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Pattern Recognit. (2026)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">Pattern Recognit.</span>
                            
                        </div>
                        <span class="date">2026</span>
                        
                            
                            <a href="https://doi.org/10.1016/j.patcog.2025.112462" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Stable Language Guidance for Vision-Language-Action Models.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Zhihao Zhan, Yuhao Chen, Jiaying Zhou, Qinhan Lv, Hao Liu, Keze Wang, Liang Lin, Guangrun Wang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">CoRR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        CoRR (2026)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                        </div>
                        <span class="date">2026</span>
                        
                            
                            <a href="https://arxiv.org/abs/2601.04052" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>CLAP: Contrastive Latent Action Pretraining for Learning Vision-Language-Action Models from Human Vi...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Chubin Zhang, Jianan Wang, Zifeng Gao, Yue Su, Tianru Dai, Cai Zhou, Jiwen Lu, Yansong Tang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">CoRR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        CoRR (2026)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                        </div>
                        <span class="date">2026</span>
                        
                            
                            <a href="https://arxiv.org/abs/2601.04061" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>PD-VLA: Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decod...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Wenxuan Song, Jiayi Chen, Pengxiang Ding, Han Zhao 0008, Wei Zhao, Zhide Zhong, Zongyuan Ge, Zhijun Li, Donglin Wang, Lujia Wang, Jun Ma 0008, Haoang Li
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IROS</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IROS (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IROS</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/IROS60139.2025.11247519" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>FAST: Efficient Action Tokenization for Vision-Language-Action Models.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair 0003, Quan Vuong, Oier Mees, Chelsea Finn, Sergey Levine
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">CoRR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        CoRR (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://arxiv.org/abs/2501.09747" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12281v1] Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-La...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Jacky Kwok, Xilun Zhang, Mengdi Xu, Yuejiang Liu, Azalia Mirhoseini, Chelsea Finn, Marco Pavone
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.RO</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">eess.SY</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the &#34;intention-action gap.&#39;&#39; We first characterize the test-time scaling law for embodied instruction following and demonstrate that...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:59:59+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12281v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12280v1] Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Huai-Hsun Cheng, Siang-Ling Zhang, Yu-Lun Liu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CV</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, a novel vector sketching task where a single sketch undergoes a dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise, a generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the &#34;dual-constraint&#34;:...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:59:54+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12280v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12279v1] UniT: Unified Multimodal Chain-of-Thought Test-time Scaling</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Leon Liangyu Chen, Haoyu Ma, Zhipeng Fan, Ziqi Huang, Animesh Sinha, Xiaoliang Dai, Jialiang Wang, Zecheng He, Jianwei Yang, Chunyuan Li, Junzhe Sun, Chu Wang, Serena Yeung-Levy, Felix Juefei-Xu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CV</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.LG</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional i...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:59:49+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12279v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12278v1] AttentionRetriever: Attention Layers are Secretly Long Document Retrievers</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: David Jiahao Fu, Lam Thanh Do, Jiayu Li, Kevin Chen-Chuan Chang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.IR</span>
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Retrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever, a novel long document retrieval model that leverages attention mechanism and entity-based...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:59:35+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12278v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12276v1] Agentic Test-Time Scaling for WebAgents</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Nicholas Lee, Lutfi Eren Erdogan, Chris Joseph John, Surya Krishnapillai, Michael W. Mahoney, Kurt Keutzer, Amir Gholami
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.CL</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Test-time scaling has become a standard way to improve performance and boost reliability of neural network models. However, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly increase sampling show diminishing returns. In this work, we present CATTS, a simple technique for dynamically allocating compute for multi-step agents. We first conduct an empirical study of inference-...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:58:30+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12276v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12274v1] Function-Space Decoupled Diffusion for Forward and Inverse Modeling in Carbon Capture...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Xin Ju, Jiachen Yao, Anima Anandkumar, Sally M. Benson, Gege Wen
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">physics.geo-ph</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Accurate characterization of subsurface flow is critical for Carbon Capture and Storage (CCS) but remains challenged by the ill-posed nature of inverse problems with sparse observations. We present Fun-DDPS, a generative framework that combines function-space diffusion models with differentiable neural operator surrogates for both forward and inverse modeling. Our approach learns a prior distribution over geological parameters (geomodel) using a single-channel diffusion model, then leverages a L...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:58:12+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12274v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12273v1] Learning to Control: The iUzawa-Net for Nonsmooth Optimal Control of Linear PDEs</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Yongcun Song, Xiaoming Yuan, Hangrui Yue, Tianyou Zeng
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">math.OC</span>
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">math.NA</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        We propose an optimization-informed deep neural network approach, named iUzawa-Net, aiming for the first solver that enables real-time solutions for a class of nonsmooth optimal control problems of linear partial differential equations (PDEs). The iUzawa-Net unrolls an inexact Uzawa method for saddle point problems, replacing classical preconditioners and PDE solvers with specifically designed learnable neural networks. We prove universal approximation properties and establish the asymptotic $\v...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:57:43+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12273v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12268v1] CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agen...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Zhen Zhang, Kaiqiang Song, Xun Wang, Yebowen Hu, Weixiang Yan, Chenyang Zhao, Henry Peng Zou, Haoyun Deng, Sathish Reddy Indurthi, Shujian Liu, Simin Ma, Xiaoyang Wang, Xin Eric Wang, Song Wang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        AI agents are increasingly used to solve real-world tasks by reasoning over multi-turn user interactions and invoking external tools. However, applying reinforcement learning to such settings remains difficult: realistic objectives often lack verifiable rewards and instead emphasize open-ended behaviors; moreover, RL for multi-turn, multi-step agentic tool use is still underexplored; and building and maintaining executable tool environments is costly, limiting scale and coverage. We propose CM2,...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:55:09+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12268v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12267v1] Self-Supervised Learning via Flow-Guided Neural Operator on Time-Series Data</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Duy Nguyen, Jiachen Yao, Jiayun Wang, Julius Berner, Animashree Anandkumar
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Self-supervised learning (SSL) is a powerful paradigm for learning from unlabeled time-series data. However, popular methods such as masked autoencoders (MAEs) rely on reconstructing inputs from a fixed, predetermined masking ratio. Instead of this static design, we propose treating the corruption level as a new degree of freedom for representation learning, enhancing flexibility and performance. To achieve this, we introduce the Flow-Guided Neural Operator (FGNO), a novel framework combining op...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:54:57+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12267v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12262v1] T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Tunyu Zhang, Xinxi Zhang, Ligong Han, Haizhou Shi, Xiaoxiao He, Zhuowei Li, Hao Wang, Kai Xu, Akash Srivastava, Hao Wang, Vladimir Pavlovic, Dimitris N. Metaxas
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CL</span>
                        
                        <span class="category">cs.LG</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to a substantial degradation in generation quality. To alleviate this, we propose a trajectory self-distillation framework that improves few-step decoding by distilling the model&#39;s own generative trajectories. We...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:52:35+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12262v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12259v1] Think like a Scientist: Physics-guided LLM Agent for Equation Discovery</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Jianke Yang, Ohm Venkatachalam, Mohammad Kianezhad, Sharvaree Vadgama, Rose Yu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.LG</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Explaining observed phenomena through symbolic, interpretable formulas is a fundamental goal of science. Recently, large language models (LLMs) have emerged as promising tools for symbolic equation discovery, owing to their broad domain knowledge and strong reasoning capabilities. However, most existing LLM-based systems try to guess equations directly from data, without modeling the multi-step reasoning process that scientists often follow: first inferring physical properties such as symmetries...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:49:27+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12259v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12253v1] Is Online Linear Optimization Sufficient for Strategic Robustness?</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Yang Cai, Haipeng Luo, Chen-Yu Wei, Weiqiang Zheng
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.GT</span>
                        
                        <span class="category">cs.LG</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        We consider bidding in repeated Bayesian first-price auctions. Bidding algorithms that achieve optimal regret have been extensively studied, but their strategic robustness to the seller&#39;s manipulation remains relatively underexplored. Bidding algorithms based on no-swap-regret algorithms achieve both desirable properties, but are suboptimal in terms of statistical and computational efficiency. In contrast, online gradient ascent is the only algorithm that achieves $O(\sqrt{TK})$ regret and strat...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:41:55+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12253v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
            
        </div>
        
        <footer>
            <p>Áî± <a href="https://github.com/your-repo/auto-researcher">AutoResearcher</a> Ëá™Âä®ÁîüÊàê</p>
        </footer>
    </div>
</body>
</html>