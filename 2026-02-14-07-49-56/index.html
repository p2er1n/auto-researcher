<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ÂÖ∑Ë∫´Êô∫ËÉΩ &amp; VLA Â≠¶ÊúØËÆ∫Êñá</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
        }
        
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px 20px;
            text-align: center;
            border-radius: 8px;
            margin-bottom: 30px;
        }
        
        header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }
        
        header p {
            font-size: 1.1em;
            opacity: 0.9;
        }
        
        .meta {
            margin-top: 15px;
            font-size: 0.9em;
            opacity: 0.8;
        }
        
        .config-info {
            background: white;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        .config-info h3 {
            color: #333;
            margin-bottom: 15px;
            font-size: 1.1em;
        }
        
        .config-info .source-list {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin-bottom: 15px;
        }
        
        .config-info .source-tag {
            background: #667eea;
            color: white;
            padding: 4px 12px;
            border-radius: 15px;
            font-size: 0.85em;
        }
        
        .config-info .filter-list {
            color: #666;
            font-size: 0.9em;
        }
        
        .config-info .filter-list strong {
            color: #333;
        }
        
        .config-info .source-stats {
            margin-top: 10px;
            padding-top: 10px;
            border-top: 1px solid #eee;
        }
        
        .config-info .source-stats strong {
            color: #333;
        }
        
        .config-info .stat-tag {
            background: #e3f2fd;
            color: #1565c0;
            padding: 3px 10px;
            border-radius: 12px;
            font-size: 0.85em;
            margin-right: 8px;
            display: inline-block;
            margin-top: 5px;
        }
        
        .items {
            display: grid;
            gap: 20px;
        }
        
        .item {
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            transition: transform 0.2s, box-shadow 0.2s;
        }
        
        .item:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
        }
        
        .item h2 {
            font-size: 1.4em;
            color: #667eea;
            margin-bottom: 10px;
        }
        
        .item .authors {
            color: #666;
            font-size: 0.9em;
            margin-bottom: 10px;
        }
        
        .item .content {
            color: #555;
            margin: 10px 0;
        }
        
        .item .footer {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-top: 15px;
            padding-top: 15px;
            border-top: 1px solid #eee;
            font-size: 0.85em;
            color: #888;
            flex-wrap: wrap;
            gap: 10px;
        }
        
        .item .source-info {
            display: flex;
            gap: 8px;
            align-items: center;
            flex-wrap: wrap;
        }
        
        .item .source-type {
            background: #667eea;
            color: white;
            padding: 3px 10px;
            border-radius: 12px;
            font-size: 0.8em;
        }
        
        .item .venue {
            background: #e8f5e9;
            color: #2e7d32;
            padding: 3px 10px;
            border-radius: 12px;
            font-size: 0.8em;
        }
        
        .item .categories {
            display: flex;
            gap: 5px;
            flex-wrap: wrap;
        }
        
        .item .category {
            background: #e3f2fd;
            color: #1565c0;
            padding: 2px 8px;
            border-radius: 4px;
            font-size: 0.8em;
        }
        
        .item .date {
            color: #666;
        }
        
        .item a {
            color: #667eea;
            text-decoration: none;
        }
        
        .item a:hover {
            text-decoration: underline;
        }
        
        .btn-pdf {
            display: inline-block;
            background: #d32f2f;
            color: white;
            padding: 5px 12px;
            border-radius: 4px;
            font-size: 0.85em;
        }
        
        .btn-pdf:hover {
            background: #b71c1c;
            text-decoration: none;
        }
        
        footer {
            text-align: center;
            padding: 30px;
            color: #888;
            font-size: 0.9em;
        }
        
        .empty {
            text-align: center;
            padding: 60px 20px;
            color: #888;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>ÂÖ∑Ë∫´Êô∫ËÉΩ &amp; VLA Â≠¶ÊúØËÆ∫Êñá</h1>
            <p>Ëá™Âä®ËøΩË∏™ CVPR, NeurIPS, ICLR, CoRL, RSS Á≠âÈ°∂‰ºöÊúÄÊñ∞ÂÖ∑Ë∫´Êô∫ËÉΩ„ÄÅVLA„ÄÅÊú∫Âô®‰∫∫Áõ∏ÂÖ≥ËÆ∫Êñá</p>
            <div class="meta">
                ÁîüÊàêÊó∂Èó¥: 2026-02-14 07:49:56
            </div>
        </header>
        
        
        <div class="config-info">
            <h3>üìã ÈÖçÁΩÆ‰ø°ÊÅØ</h3>
            <div class="source-list">
                <strong>Êï∞ÊçÆÊ∫ê:</strong>
                
                    
                    
                        
                        <span class="source-tag">DBLP</span>
                        
                        
                    
                        
                        
                    
                        
                        <span class="source-tag">arXiv</span>
                        
                        
                    
                        
                        
                    
                
            </div>
            
            
            
            <div class="source-stats">
                <strong>üìä Êù•Ê∫êÁªüËÆ°:</strong>
                
                <span class="stat-tag">DBLP RSS: 3 ÁØá</span>
                
                <span class="stat-tag">arXiv Library: 61 ÁØá</span>
                
            </div>
            
            
            <div class="filter-list">
                <strong>ÂÖ≥ÈîÆËØçËøáÊª§:</strong>
                
                    
                        
                    
                        
                            VLA, Vision-Language-Action, Vision Language Action, embodied, embodied intelligence, ÂÖ∑Ë∫´Êô∫ËÉΩ, robot, robotics, robotic, Êú∫Âô®‰∫∫, manipulation, grasping, dexterous manipulation, navigation, visual navigation, language navigation, humanoid, android, ‰ªø‰∫∫Êú∫Âô®‰∫∫, sim-to-real, domain randomization, sim2real, reinforcement learning, RL, Âº∫ÂåñÂ≠¶‰π†, imitation learning, behavior cloning, Ê®°‰ªøÂ≠¶‰π†, LLM robot, large language model robot, ËØ≠Ë®ÄÊ®°ÂûãÊú∫Âô®‰∫∫, multimodal, multimodal learning, foundation model, visual foundation model, scene understanding, 3D understanding, object detection, object recognition, semantic understanding, scene graph
                        
                    
                        
                    
                        
                    
                
            </div>
        </div>
        
        <div class="items">
            
                
                
                <div class="item">
                    <h2>Position: Language model developers should report train-test overlap.</h2>
                    
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ICML</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ICML 
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ICML</span>
                            
                        </div>
                        <span class="date"></span>
                        
                            
                            <a href="https://dblp.org/rec/conf/icml/ZhangK0LZBL25" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Position: Lifetime tuning is incompatible with continual reinforcement learning.</h2>
                    
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ICML</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ICML 
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ICML</span>
                            
                        </div>
                        <span class="date"></span>
                        
                            
                            <a href="https://dblp.org/rec/conf/icml/MesbahiPMTW025" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Position: Constants are Critical in Regret Bounds for Reinforcement Learning.</h2>
                    
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ICML</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ICML 
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ICML</span>
                            
                        </div>
                        <span class="date"></span>
                        
                            
                            <a href="https://dblp.org/rec/conf/icml/DragoMM25b" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12281v1] Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-La...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Jacky Kwok, Xilun Zhang, Mengdi Xu, Yuejiang Liu, Azalia Mirhoseini, Chelsea Finn, Marco Pavone
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.RO</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">eess.SY</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the &#34;intention-action gap.&#39;&#39; We first characterize the test-time scaling law for embodied instruction following and demonstrate that...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:59:59+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12281v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12280v1] Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Huai-Hsun Cheng, Siang-Ling Zhang, Yu-Lun Liu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CV</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, a novel vector sketching task where a single sketch undergoes a dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise, a generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the &#34;dual-constraint&#34;:...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:59:54+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12280v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12279v1] UniT: Unified Multimodal Chain-of-Thought Test-time Scaling</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Leon Liangyu Chen, Haoyu Ma, Zhipeng Fan, Ziqi Huang, Animesh Sinha, Xiaoliang Dai, Jialiang Wang, Zecheng He, Jianwei Yang, Chunyuan Li, Junzhe Sun, Chu Wang, Serena Yeung-Levy, Felix Juefei-Xu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CV</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.LG</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional i...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:59:49+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12279v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12268v1] CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agen...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Zhen Zhang, Kaiqiang Song, Xun Wang, Yebowen Hu, Weixiang Yan, Chenyang Zhao, Henry Peng Zou, Haoyun Deng, Sathish Reddy Indurthi, Shujian Liu, Simin Ma, Xiaoyang Wang, Xin Eric Wang, Song Wang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        AI agents are increasingly used to solve real-world tasks by reasoning over multi-turn user interactions and invoking external tools. However, applying reinforcement learning to such settings remains difficult: realistic objectives often lack verifiable rewards and instead emphasize open-ended behaviors; moreover, RL for multi-turn, multi-step agentic tool use is still underexplored; and building and maintaining executable tool environments is costly, limiting scale and coverage. We propose CM2,...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:55:09+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12268v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12253v1] Is Online Linear Optimization Sufficient for Strategic Robustness?</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Yang Cai, Haipeng Luo, Chen-Yu Wei, Weiqiang Zheng
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.GT</span>
                        
                        <span class="category">cs.LG</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        We consider bidding in repeated Bayesian first-price auctions. Bidding algorithms that achieve optimal regret have been extensively studied, but their strategic robustness to the seller&#39;s manipulation remains relatively underexplored. Bidding algorithms based on no-swap-regret algorithms achieve both desirable properties, but are suboptimal in terms of statistical and computational efficiency. In contrast, online gradient ascent is the only algorithm that achieves $O(\sqrt{TK})$ regret and strat...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:41:55+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12253v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12249v1] &#34;Sorry, I Didn&#39;t Catch That&#34;: How Speech Models Miss What Matters Most</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Kaitlyn Zhou, Martijn Bartelds, Federico Bianchi, James Zou
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.CL</span>
                        
                        <span class="category">cs.CY</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Despite speech recognition systems achieving low word error rates on standard benchmarks, they often fail on short, high-stakes utterances in real-world deployments. Here, we study this failure mode in a high-stakes task: the transcription of U.S. street names as spoken by U.S. participants. We evaluate 15 models from OpenAI, Deepgram, Google, and Microsoft on recordings from linguistically diverse U.S. speakers and find an average transcription error rate of 44%. We quantify the downstream impa...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:36:09+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12249v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12246v1] 6G Empowering Future Robotics: A Vision for Next-Generation Autonomous Systems</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Mona Ghassemian, Andr√©s Meseguer Valenzuela, Ana Garcia Armada, Dejan Vukobratovic, Periklis Chatzimisios, Kaspar Althoefer, Ranga Rao Venkatesha Prasad
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.NI</span>
                        
                        <span class="category">cs.RO</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        The convergence of robotics and next-generation communication is a critical driver of technological advancement. As the world transitions from 5G to 6G, the foundational capabilities of wireless networks are evolving to support increasingly complex and autonomous robotic systems. This paper examines the transformative impact of 6G on enhancing key robotics functionalities. It provides a systematic mapping of IMT-2030 key performance indicators to robotic functional blocks including sensing, perc...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:31:24+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12246v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12245v1] Intrinsic-Energy Joint Embedding Predictive Architectures Induce Quasimetric Spaces</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Anthony Kobanda, Waris Radji
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Joint-Embedding Predictive Architectures (JEPAs) aim to learn representations by predicting target embeddings from context embeddings, inducing a scalar compatibility energy in a latent space. In contrast, Quasimetric Reinforcement Learning (QRL) studies goal-conditioned control through directed distance values (cost-to-go) that support reaching goals under asymmetric dynamics. In this short article, we connect these viewpoints by restricting attention to a principled class of JEPA energy functi...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:30:27+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12245v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12244v1] Any House Any Task: Scalable Long-Horizon Planning for Abstract Human Tasks</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Zhihong Liu, Yang Li, Rengming Huang, Cewu Lu, Panpan Cai
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.RO</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Open world language conditioned task planning is crucial for robots operating in large-scale household environments. While many recent works attempt to address this problem using Large Language Models (LLMs) via prompting or training, a key challenge remains scalability. Performance often degrades rapidly with increasing environment size, plan length, instruction ambiguity, and constraint complexity. In this work, we propose Any House Any Task (AHAT), a household task planner optimized for long-...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:28:28+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12244v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12241v1] Moonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Manjunath Kudlur, Evan King, James Wang, Pete Warden
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CL</span>
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">cs.SD</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Latency-critical speech applications (e.g., live transcription, voice commands, and real-time translation) demand low time-to-first-token (TTFT) and high transcription accuracy, particularly on resource-constrained edge devices. Full-attention Transformer encoders remain a strong accuracy baseline for automatic speech recognition (ASR) because every frame can directly attend to every other frame, which resolves otherwise locally ambiguous acoustics using distant lexical context. However, this gl...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:20:45+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12241v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12237v1] Olmix: A Framework for Data Mixing Throughout LM Development</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Mayee F. Chen, Tyler Murray, David Heineman, Matt Jordan, Hannaneh Hajishirzi, Christopher R√©, Luca Soldaini, Kyle Lo
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.CL</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Data mixing -- determining the ratios of data from different domains -- is a first-order concern for training language models (LMs). While existing mixing methods show promise, they fall short when applied during real-world LM development. We present Olmix, a framework that addresses two such challenges. First, the configuration space for developing a mixing method is not well understood -- design choices across existing methods lack justification or consensus and overlook practical issues like...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:16:05+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12237v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12236v1] Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural Networks for Ne...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Anika Tabassum Meem, Muntasir Hossain Nadid, Md Zesun Ahmed Mia
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.NE</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.CV</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Neuromorphic vision systems based on spiking neural networks (SNNs) offer ultra-low-power perception for event-based and frame-based cameras, yet catastrophic forgetting remains a critical barrier to deployment in continually evolving environments. Existing continual learning methods, developed primarily for artificial neural networks, seldom jointly optimize accuracy and energy efficiency, with particularly limited exploration on event-based datasets. We propose an energy-aware spike budgeting...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:15:32+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12236v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12229v1] Diffusion Alignment Beyond KL: Variance Minimisation as Effective Policy Optimiser</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Zijing Ou, Jacob Si, Junyi Zhu, Ondrej Bohdal, Mete Ozay, Taha Ceritli, Yingzhen Li
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Diffusion alignment adapts pretrained diffusion models to sample from reward-tilted distributions along the denoising trajectory. This process naturally admits a Sequential Monte Carlo (SMC) interpretation, where the denoising model acts as a proposal and reward guidance induces importance weights. Motivated by this view, we introduce Variance Minimisation Policy Optimisation (VMPO), which formulates diffusion alignment as minimising the variance of log importance weights rather than directly op...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:06:03+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12229v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12224v1] Bandit Learning in Matching Markets with Interviews</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Amirmahdi Mirfakhar, Xuchuang Wang, Mengfan Xu, Hedyeh Beyhaghi, Mohammad Hajiesmaili
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.GT</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">econ.TH</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Two-sided matching markets rely on preferences from both sides, yet it is often impractical to evaluate preferences. Participants, therefore, conduct a limited number of interviews, which provide early, noisy impressions and shape final decisions. We study bandit learning in matching markets with interviews, modeling interviews as \textit{low-cost hints} that reveal partial preference information to both sides. Our framework departs from existing work by allowing firm-side uncertainty: firms, li...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:03:37+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12224v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12222v1] Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM T...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Miaosen Zhang, Yishan Liu, Shuxia Lin, Xu Yang, Qi Dai, Chong Luo, Weihao Jiang, Peng Hou, Anxiang Zeng, Xin Geng, Baining Guo
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.CV</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Supervised fine-tuning (SFT) is computationally efficient but often yields inferior generalization compared to reinforcement learning (RL). This gap is primarily driven by RL&#39;s use of on-policy data. We propose a framework to bridge this chasm by enabling On-Policy SFT. We first present \textbf{\textit{Distribution Discriminant Theory (DDT)}}, which explains and quantifies the alignment between data and the model-induced distribution. Leveraging DDT, we introduce two complementary techniques: (i...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:59:58+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12222v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12221v1] Best of Both Worlds: Multimodal Reasoning and Generation via Unified Discrete Flow Ma...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Onkar Susladkar, Tushar Prakash, Gayatri Deshmukh, Kiet A. Nguyen, Jiaxun Zhang, Adheesh Juvekar, Tianshu Bao, Lin Chai, Sparsh Mittal, Inderjit S Dhillon, Ismini Lourentzou
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CV</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        We propose UniDFlow, a unified discrete flow-matching framework for multimodal understanding, generation, and editing. It decouples understanding and generation via task-specific low-rank adapters, avoiding objective interference and representation entanglement, while a novel reference-based multimodal preference alignment optimizes relative outcomes under identical conditioning, improving faithfulness and controllability without large-scale retraining. UniDFlpw achieves SOTA performance across...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:59:08+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12221v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12218v1] The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Christian Intern√≤, Jumpei Yamaguchi, Loren Amdahl-Culleton, Markus Olhofer, David Klindt, Barbara Hammer
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Determining whether neural models internalize physical laws as world models, rather than exploiting statistical shortcuts, remains challenging, especially under out-of-distribution (OOD) shifts. Standard evaluations often test latent capability via downstream adaptation (e.g., fine-tuning or high-capacity probes), but such interventions can change the representations being measured and thus confound what was learned during self-supervised learning (SSL). We propose a non-invasive evaluation prot...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:56:07+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12218v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12215v1] LDA-1B: Scaling Latent Dynamics Action Model via Universal Embodied Data Ingestion</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Jiangran Lyu, Kai Liu, Xuheng Zhang, Haoran Liao, Yusen Feng, Wenxuan Zhu, Tingrui Shen, Jiayi Chen, Jiazhao Zhang, Yifei Dong, Wenbo Cui, Senmao Qi, Shuo Wang, Yixin Zheng, Mi Yan, Xuesong Shi, Haoran Li, Dongbin Zhao, Ming-Yu Liu, Zhizheng Zhang, Li Yi, Yizhou Wang, He Wang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.RO</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Recent robot foundation models largely rely on large-scale behavior cloning, which imitates expert actions but discards transferable dynamics knowledge embedded in heterogeneous embodied data. While the Unified World Model (UWM) formulation has the potential to leverage such diverse data, existing instantiations struggle to scale to foundation-level due to coarse data usage and fragmented datasets. We introduce LDA-1B, a robot foundation model that scales through universal embodied data ingestio...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:53:51+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12215v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12207v1] VIRENA: Virtual Arena for Research, Education, and Democratic Innovation</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Emma Hoes, K. Jonathan Klueser, Fabrizio Gilardi
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.HC</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.SI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Digital platforms shape how people communicate, deliberate, and form opinions. Studying these dynamics has become increasingly difficult due to restricted data access, ethical constraints on real-world experiments, and limitations of existing research tools. VIRENA (Virtual Arena) is a platform that enables controlled experimentation in realistic social media environments. Multiple participants interact simultaneously in realistic replicas of feed-based platforms (Instagram, Facebook, Reddit) an...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:46:52+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12207v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12205v1] DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation an...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Dianyi Wang, Ruihang Li, Feng Han, Chaofan Ma, Wei Song, Siyuan Wang, Yibin Wang, Yi Xin, Hongjian Liu, Zhixiong Zhang, Shengyuan Ding, Tianhang Wang, Zhenglin Cheng, Tao Lin, Cheng Jin, Kaicheng Yu, Jingjing Chen, Wenjie Wang, Zhongyu Wei, Jiaqi Wang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CV</span>
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., &gt;10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:44:24+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12205v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12203v1] ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from D...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Mathieu Sibue, Andres Mu√±oz Garza, Samuel Mensah, Pranav Shetty, Zhiqiang Ma, Xiaomo Liu, Manuela Veloso
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CL</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Enterprise documents, such as forms and reports, embed critical information for downstream applications like data archiving, automated workflows, and analytics. Although generalist Vision Language Models (VLMs) perform well on established document understanding benchmarks, their ability to conduct holistic, fine-grained structured extraction across diverse document types and flexible schemas is not well studied. Existing Key Entity Extraction (KEE), Relation Extraction (RE), and Visual Question...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:38:57+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12203v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12199v1] Sub--Riemannian boundary value problems for Optimal Geometric Locomotion</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Oliver Gross, Florine Hartwig, Martin Rumpf, Peter Schr√∂der
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.RO</span>
                        
                        <span class="category">math.NA</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        We propose a geometric model for optimal shape-change-induced motions of slender locomotors, e.g., snakes slithering on sand. In these scenarios, the motion of a body in world coordinates is completely determined by the sequence of shapes it assumes. Specifically, we formulate Lagrangian least-dissipation principles as boundary value problems whose solutions are given by sub-Riemannian geodesics. Notably, our geometric model accounts not only for the energy dissipated by the body&#39;s displacement...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:32:20+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12199v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12196v1] Visual Reasoning Benchmark: Evaluating Multimodal LLMs on Classroom-Authentic Visual...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Mohamed Huti, Alasdair Mackintosh, Amy Waldock, Dominic Andrews, Maxime Leli√®vre, Moritz Boos, Tobias Murray, Paul Atherton, Robin A. A. Ince, Oliver G. B. Garrod
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CL</span>
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        AI models have achieved state-of-the-art results in textual reasoning; however, their ability to reason over spatial and relational structures remains a critical bottleneck -- particularly in early-grade maths, which relies heavily on visuals. This paper introduces the visual reasoning benchmark (VRB), a novel dataset designed to evaluate Multimodal Large Language Models (MLLMs) on their ability to solve authentic visual problems from classrooms. This benchmark is built on a set of 701 questions...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:29:03+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12196v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12189v1] WaveFormer: Wavelet Embedding Transformer for Biomedical Signals</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Habib Irani, Bikram De, Vangelis Metsis
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Biomedical signal classification presents unique challenges due to long sequences, complex temporal dynamics, and multi-scale frequency patterns that are poorly captured by standard transformer architectures. We propose WaveFormer, a transformer architecture that integrates wavelet decomposition at two critical stages: embedding construction, where multi-channel Discrete Wavelet Transform (DWT) extracts frequency features to create tokens containing both time-domain and frequency-domain informat...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:20:43+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12189v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12187v1] SAGEO Arena: A Realistic Environment for Evaluating Search-Augmented Generative Engin...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Sunghwan Kim, Wooseok Jeong, Serin Kim, Sangam Lee, Dongha Lee
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.IR</span>
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Search-Augmented Generative Engines (SAGE) have emerged as a new paradigm for information access, bridging web-scale retrieval with generative capabilities to deliver synthesized answers. This shift has fundamentally reshaped how web content gains exposure online, giving rise to Search-Augmented Generative Engine Optimization (SAGEO), the practice of optimizing web documents to improve their visibility in AI-generated responses. Despite growing interest, no evaluation environment currently suppo...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:18:00+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12187v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12181v1] Convex Markov Games and Beyond: New Proof of Existence, Characterization and Learning...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Anas Barakat, Ioannis Panageas, Antonios Varvitsiotis
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.GT</span>
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">cs.MA</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Convex Markov Games (cMGs) were recently introduced as a broad class of multi-agent learning problems that generalize Markov games to settings where strategic agents optimize general utilities beyond additive rewards. While cMGs expand the modeling frontier, their theoretical foundations, particularly the structure of Nash equilibria (NE) and guarantees for learning algorithms, are not yet well understood. In this work, we address these gaps for an extension of cMGs, which we term General Utilit...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:11:20+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12181v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12180v1] How Sampling Shapes LLM Alignment: From One-Shot Optima to Iterative Dynamics</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Yurong Chen, Yu He, Michael I. Jordan, Fan Yao
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">cs.GT</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Standard methods for aligning large language models with human preferences learn from pairwise comparisons among sampled candidate responses and regularize toward a reference policy. Despite their effectiveness, the effects of sampling and reference choices are poorly understood theoretically. We investigate these effects through Identity Preference Optimization, a widely used preference alignment framework, and show that proper instance-dependent sampling can yield stronger ranking guarantees,...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:11:08+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12180v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12172v1] Pedagogically-Inspired Data Synthesis for Language Model Knowledge Distillation</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Bowei He, Yankai Chen, Xiaokun Zhang, Linghe Kong, Philip S. Yu, Xue Liu, Chen Ma
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.CL</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Knowledge distillation from Large Language Models (LLMs) to smaller models has emerged as a critical technique for deploying efficient AI systems. However, current methods for distillation via synthetic data lack pedagogical awareness, treating knowledge transfer as a one-off data synthesis and training task rather than a systematic learning process. In this paper, we propose a novel pedagogically-inspired framework for LLM knowledge distillation that draws from fundamental educational principle...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:00:36+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12172v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12170v1] Statistical Parsing for Logical Information Retrieval</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Greg Coppola
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        In previous work (Coppola, 2024) we introduced the Quantified Boolean Bayesian Network (QBBN), a logical graphical model that implements the forward fragment of natural deduction (Prawitz, 1965) as a probabilistic factor graph. That work left two gaps: no negation/backward reasoning, and no parser for natural language.
  This paper addresses both gaps across inference, semantics, and syntax. For inference, we extend the QBBN with NEG factors enforcing P(x) + P(neg x) = 1, enabling contrapositive...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:57:25+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12170v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12160v1] DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Xu Guo, Fulong Ye, Qichao Sun, Liyang Chen, Bingchuan Li, Pengze Zhang, Jiawei Liu, Songtao Zhao, Qian He, Xiangwang Hou
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CV</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni, a...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:41:52+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12160v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12159v1] 3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Wancai Zheng, Hao Chen, Xianlong Lu, Linlin Ou, Xinyi Yu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.RO</span>
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Object navigation is a core capability of embodied intelligence, enabling an agent to locate target objects in unknown environments. Recent advances in vision-language models (VLMs) have facilitated zero-shot object navigation (ZSON). However, existing methods often rely on scene abstractions that convert environments into semantic maps or textual representations, causing high-level decision making to be constrained by the accuracy of low-level perception. In this work, we present 3DGSNav, a nov...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:41:26+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12159v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12158v1] SafeNeuron: Neuron-Level Safety Alignment for Large Language Models</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Zhaoxin Wang, Jiaming Liang, Fengbin Zhu, Weixiang Zhao, Junfeng Fang, Jiayi Ji, Handing Wang, Tat-Seng Chua
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Large language models (LLMs) and multimodal LLMs are typically safety-aligned before release to prevent harmful content generation. However, recent studies show that safety behaviors are concentrated in a small subset of parameters, making alignment brittle and easily bypassed through neuron-level attacks. Moreover, most existing alignment methods operate at the behavioral level, offering limited control over the model&#39;s internal safety mechanisms. In this work, we propose SafeNeuron, a neuron-l...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:40:05+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12158v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12155v1] FAIL: Flow Matching Adversarial Imitation Learning for Image Generation</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Yeyao Ma, Chen Li, Xiaosong Zhang, Han Hu, Weidi Xie
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CV</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Post-training of flow matching models-aligning the output distribution with a high-quality target-is mathematically equivalent to imitation learning. While Supervised Fine-Tuning mimics expert demonstrations effectively, it cannot correct policy drift in unseen states. Preference optimization methods address this but require costly preference pairs or reward modeling. We propose Flow Matching Adversarial Imitation Learning (FAIL), which minimizes policy-expert divergence through adversarial trai...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:36:33+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12155v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12147v1] It&#39;s TIME: Towards the Next Generation of Time Series Forecasting Benchmarks</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Zhongzheng Qiao, Sheng Pan, Anni Wang, Viktoriya Zhukova, Yong Liu, Xudong Jiang, Qingsong Wen, Mingsheng Long, Ming Jin, Chenghao Liu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Time series foundation models (TSFMs) are revolutionizing the forecasting landscape from specific dataset modeling to generalizable task evaluation. However, we contend that existing benchmarks exhibit common limitations in four dimensions: constrained data composition dominated by reused legacy sources, compromised data integrity lacking rigorous quality assurance, misaligned task formulations detached from real-world contexts, and rigid analysis perspectives that obscure generalizable insights...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:31:01+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12147v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12146v1] Seq2Seq2Seq: Lossless Data Compression via Discrete Latent Transformers and Reinforce...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Mahdi Khodabandeh, Ghazal Shabani, Arash Yousefi Jordehi, Seyed Abolghasem Mirroshandel
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.CL</span>
                        
                        <span class="category">cs.IT</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Efficient lossless compression is essential for minimizing storage costs and transmission overhead while preserving data integrity. Traditional compression techniques, such as dictionary-based and statistical methods, often struggle to optimally exploit the structure and redundancy in complex data formats. Recent advancements in deep learning have opened new avenues for compression; however, many existing approaches depend on dense vector representations that obscure the underlying token structu...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:30:55+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12146v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12144v1] On the Adoption of AI Coding Agents in Open-source Android and iOS Development</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Muhammad Ahmad Khan, Hasnain Ali, Muneeb Rana, Muhammad Saqib Ilyas, Abdul Ali Bangash
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.SE</span>
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        AI coding agents are increasingly contributing to software development, yet their impact on mobile development has received little empirical attention. In this paper, we present the first category-level empirical study of agent-generated code in open-source mobile app projects. We analyzed PR acceptance behaviors across mobile platforms, agents, and task categories using 2,901 AI-authored pull requests (PRs) in 193 verified Android and iOS open-source GitHub repositories in the AIDev dataset. We...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:30:29+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12144v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12137v1] CitiLink-Minutes: A Multilayer Annotated Dataset of Municipal Meeting Minutes</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Ricardo Campos, Ana Filipa Pacheco, Ana Lu√≠sa Fernandes, In√™s Cantante, Rute Rebou√ßas, Lu√≠s Filipe Cunha, Jos√© Miguel Isidro, Jos√© Pedro Evans, Miguel Marques, Rodrigo Batista, Evelin Amorim, Al√≠pio Jorge, Nuno Guimar√£es, S√©rgio Nunes, Ant√≥nio Leal, Purifica√ß√£o Silvano
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CL</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        City councils play a crucial role in local governance, directly influencing citizens&#39; daily lives through decisions made during municipal meetings. These deliberations are formally documented in meeting minutes, which serve as official records of discussions, decisions, and voting outcomes. Despite their importance, municipal meeting records have received little attention in Information Retrieval (IR) and Natural Language Processing (NLP), largely due to the lack of annotated datasets, which ult...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:22:55+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12137v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12135v1] WavBench: Benchmarking Reasoning, Colloquialism, and Paralinguistics for End-to-End S...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Yangzhuo Li, Shengpeng Ji, Yifu Chen, Tianle Liang, Haorong Ying, Yule Wang, Junbo Li, Jun Fang, Zhou Zhao
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CL</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        With the rapid integration of advanced reasoning capabilities into spoken dialogue models, the field urgently demands benchmarks that transcend simple interactions to address real-world complexity. However, current evaluations predominantly adhere to text-generation standards, overlooking the unique audio-centric characteristics of paralinguistics and colloquialisms, alongside the cognitive depth required by modern agents. To bridge this gap, we introduce WavBench, a comprehensive benchmark desi...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:22:11+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12135v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12133v1] Neutral Prompts, Non-Neutral People: Quantifying Gender and Skin-Tone Bias in Gemini...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Roberto Balestri
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.CL</span>
                        
                        <span class="category">cs.CY</span>
                        
                        <span class="category">cs.HC</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        This study quantifies gender and skin-tone bias in two widely deployed commercial image generators - Gemini Flash 2.5 Image (NanoBanana) and GPT Image 1.5 - to test the assumption that neutral prompts yield demographically neutral outputs. We generated 3,200 photorealistic images using four semantically neutral prompts. The analysis employed a rigorous pipeline combining hybrid color normalization, facial landmark masking, and perceptually uniform skin tone quantification using the Monk (MST), P...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:21:03+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12133v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12125v1] Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Wenkai Yang, Weijie Liu, Ruobing Xie, Kai Yang, Saiyong Yang, Yankai Lin
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.CL</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        On-policy distillation (OPD), which aligns the student with the teacher&#39;s logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:14:29+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12125v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12124v1] Capability-Oriented Training Induced Alignment Risk</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Yujun Zhou, Yue Huang, Han Bao, Kehan Guo, Zhenwen Liang, Pin-Yu Chen, Tian Gao, Werner Geyer, Nuno Moniz, Nitesh V Chawla, Xiangliang Zhang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">cs.CL</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        While most AI alignment research focuses on preventing models from generating explicitly harmful content, a more subtle risk is emerging: capability-oriented training induced exploitation. We investigate whether language models, when trained with reinforcement learning (RL) in environments with implicit loopholes, will spontaneously learn to exploit these flaws to maximize their reward, even without any malicious intent in their training. To test this, we design a suite of four diverse &#34;vulnerab...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:13:14+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12124v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12123v1] Meta-Sel: Efficient Demonstration Selection for In-Context Learning via Supervised Me...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Xubin Wang, Weijia Jia
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.CL</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Demonstration selection is a practical bottleneck in in-context learning (ICL): under a tight prompt budget, accuracy can change substantially depending on which few-shot examples are included, yet selection must remain cheap enough to run per query over large candidate pools. We propose Meta-Sel, a lightweight supervised meta-learning approach for intent classification that learns a fast, interpretable scoring function for (candidate, query) pairs from labeled training data.
  Meta-Sel construc...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:11:29+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12123v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12120v1] Commencing-Student Enrolment Forecasting Under Data Sparsity with Time Series Foundat...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Jittarin Jetwiriyanon, Teo Susnjak, Surangika Ranathunga
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Many universities face increasing financial pressure and rely on accurate forecasts of commencing enrolments. However, enrolment forecasting in higher education is often data-sparse; annual series are short and affected by reporting changes and regime shifts. Popular classical approaches can be unreliable, as parameter estimation and model selection are unstable with short samples, and structural breaks degrade extrapolation. Recently, TSFMs have provided zero-shot priors, delivering strong gain...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:10:42+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12120v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12117v1] KAN-FIF: Spline-Parameterized Lightweight Physics-based Tropical Cyclone Estimation o...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Jiakang Shen, Qinghui Chen, Runtong Wang, Chenrui Xu, Jinglin Zhang, Cong Bai, Feng Zhang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Tropical cyclones (TC) are among the most destructive natural disasters, causing catastrophic damage to coastal regions through extreme winds, heavy rainfall, and storm surges. Timely monitoring of tropical cyclones is crucial for reducing loss of life and property, yet it is hindered by the computational inefficiency and high parameter counts of existing methods on resource-constrained edge devices. Current physics-guided models suffer from linear feature interactions that fail to capture high-...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:07:39+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12117v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12116v1] P-GenRM: Personalized Generative Reward Model with Test-time User-based Scaling</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Pinyi Zhang, Ting-En Lin, Yuchuan Wu, Jingyang Chen, Zongqi Wang, Hua Yang, Ze Xu, Fei Huang, Kai Zhang, Yongbin Li
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CL</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Personalized alignment of large language models seeks to adapt responses to individual user preferences, typically via reinforcement learning. A key challenge is obtaining accurate, user-specific reward signals in open-ended scenarios. Existing personalized reward models face two persistent limitations: (1) oversimplifying diverse, scenario-specific preferences into a small, fixed set of evaluation principles, and (2) struggling with generalization to new users with limited feedback. To this end...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:07:22+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12116v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12113v1] Stop Unnecessary Reflection: Training LRMs for Efficient Reasoning with Adaptive Refl...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Zewei Yu, Lirong Gao, Yuke Zhu, Bo Zheng, Sheng Guo, Haobo Wang, Junbo Zhao
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.CL</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks by employing test-time scaling. However, they often generate over-long chains-of-thought that, driven by substantial reflections such as repetitive self-questioning and circular reasoning, lead to high token consumption, substantial computational overhead, and increased latency without improving accuracy, particularly in smaller models. Our observation reveals that increasing problem complexity indu...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:04:00+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12113v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12112v1] Few-Shot Design Optimization by Exploiting Auxiliary Information</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Arjun Mani, Carl Vondrick, Richard Zemel
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Many real-world design problems involve optimizing an expensive black-box function $f(x)$, such as hardware design or drug discovery. Bayesian Optimization has emerged as a sample-efficient framework for this problem. However, the basic setting considered by these methods is simplified compared to real-world experimental setups, where experiments often generate a wealth of useful information. We introduce a new setting where an experiment generates high-dimensional auxiliary information $h(x)$ a...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:03:46+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12112v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12108v1] The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Xiaoyuan Liu, Tian Liang, Dongyang Ma, Deyu Zhou, Haitao Mi, Pinjia He, Yan Wang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        In the world of Harry Potter, when Dumbledore&#39;s mind is overburdened, he extracts memories into a Pensieve to be revisited later. In the world of AI, while we possess the Pensieve-mature databases and retrieval systems, our models inexplicably lack the &#34;wand&#34; to operate it. They remain like a Dumbledore without agency, passively accepting a manually engineered context as their entire memory. This work finally places the wand in the model&#39;s hand. We introduce StateLM, a new class of foundation mo...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:00:01+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12108v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12107v1] On the Complexity of Offline Reinforcement Learning with $Q^\star$-Approximation and...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Haolin Liu, Braham Snyder, Chen-Yu Wei
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">stat.ML</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        We study offline reinforcement learning under $Q^\star$-approximation and partial coverage, a setting that motivates practical algorithms such as Conservative $Q$-Learning (CQL; Kumar et al., 2020) but has received limited theoretical attention. Our work is inspired by the following open question: &#34;Are $Q^\star$-realizability and Bellman completeness sufficient for sample-efficient offline RL under partial coverage?&#34;
  We answer in the negative by establishing an information-theoretic lower boun...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:59:42+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12107v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12100v1] AssetFormer: Modular 3D Assets Generation with Autoregressive Transformer</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Lingting Zhu, Shengju Qian, Haidi Fan, Jiayu Dong, Zhenchao Jin, Siwei Zhou, Gen Dong, Xin Wang, Lequan Yu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CV</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        The digital industry demands high-quality, diverse modular 3D assets, especially for user-generated content~(UGC). In this work, we introduce AssetFormer, an autoregressive Transformer-based model designed to generate modular 3D assets from textual descriptions. Our pilot study leverages real-world modular assets collected from online platforms. AssetFormer tackles the challenge of creating assets composed of primitives that adhere to constrained design parameters for various applications. By in...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:55:21+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12100v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12099v1] GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: GigaBrain Team, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Hao Li, Jie Li, Jindi Lv, Jingyu Liu, Lv Feng, Mingming Yu, Peng Li, Qiuping Deng, Tianze Liu, Xinyu Zhou, Xinze Chen, Xiaofeng Wang, Yang Wang, Yifan Li, Yifei Nie, Yilong Li, Yukun Zhou, Yun Ye, Zhichao Liu, Zheng Zhu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CV</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Vision-language-action (VLA) models that directly predict multi-step action chunks from current observations face inherent limitations due to constrained scene understanding and weak future anticipation capabilities. In contrast, video world models pre-trained on web-scale video corpora exhibit robust spatiotemporal reasoning and accurate future prediction, making them a natural foundation for enhancing VLA learning. Therefore, we propose \textit{GigaBrain-0.5M*}, a VLA model trained via world m...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:55:19+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12099v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12096v1] Multi Graph Search for High-Dimensional Robot Motion Planning</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Itamar Mishani, Maxim Likhachev
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.RO</span>
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Efficient motion planning for high-dimensional robotic systems, such as manipulators and mobile manipulators, is critical for real-time operation and reliable deployment. Although advances in planning algorithms have enhanced scalability to high-dimensional state spaces, these improvements often come at the cost of generating unpredictable, inconsistent motions or requiring excessive computational resources and memory. In this work, we introduce Multi-Graph Search (MGS), a search-based motion pl...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:50:15+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12096v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12092v1] DeepSight: An All-in-One LM Safety Toolkit</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Bo Zhang, Jiaxuan Guo, Lijun Li, Dongrui Liu, Sujin Chen, Guanxu Chen, Zhijie Zheng, Qihao Lin, Lewen Yan, Chen Qian, Yijin Zhou, Yuyao Wu, Shaoxiong Guo, Tianyi Du, Jingyi Yang, Xuhao Hu, Ziqi Miao, Xiaoya Lu, Jing Shao, Xia Hu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CL</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.CR</span>
                        
                        <span class="category">cs.CV</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluation can only locate external behavioral risks but cannot figure out internal root causes. Meanwhile, safety diagnosis often drifts from concrete risk scenarios and remains at the explainable level. In t...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:43:14+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12092v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12089v1] Choose Your Agent: Tradeoffs in Adopting AI Advisors, Coaches, and Delegates in Multi...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Kehang Zhu, Lithium Thain, Vivian Tsai, James Wexler, Crystal Qian
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.GT</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.HC</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        As AI usage becomes more prevalent in social contexts, understanding agent-user interaction is critical to designing systems that improve both individual and group outcomes. We present an online behavioral experiment (N = 243) in which participants play three multi-turn bargaining games in groups of three. Each game, presented in randomized order, grants \textit{access to} a single LLM assistance modality: proactive recommendations from an \textit{Advisor}, reactive feedback from a \textit{Coach...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:41:57+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12089v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12087v1] Geometry of Uncertainty: Learning Metric Spaces for Multimodal State Estimation in RL</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Alfredo Reichlin, Adriano Pacciarelli, Danica Kragic, Miguel Vasco
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Estimating the state of an environment from high-dimensional, multimodal, and noisy observations is a fundamental challenge in reinforcement learning (RL). Traditional approaches rely on probabilistic models to account for the uncertainty, but often require explicit noise assumptions, in turn limiting generalization. In this work, we contribute a novel method to learn a structured latent representation, in which distances between states directly correlate with the minimum number of actions requi...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:41:20+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12087v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12074v1] RF-Modulated Adaptive Communication Improves Multi-Agent Robotic Exploration</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Lorin Achey, Breanne Crockett, Christoffer Heckman, Bradley Hayes
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.RO</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Reliable coordination and efficient communication are critical challenges for multi-agent robotic exploration of environments where communication is limited. This work introduces Adaptive-RF Transmission (ART), a novel communication-aware planning algorithm that dynamically modulates transmission location based on signal strength and data payload size, enabling heterogeneous robot teams to share information efficiently without unnecessary backtracking. We further explore an extension to this app...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:33:17+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12074v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12065v1] Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied L...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Xiang Liu, Sen Cui, Guocai Yao, Zhong Cao, Jingheng Ma, Min Zhang, Changshui Zhang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.RO</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Training robotic policies directly in the real world is expensive and unscalable. Although generative simulation enables large-scale data synthesis, current approaches often fail to generate logically coherent long-horizon tasks and struggle with dynamic physical uncertainties due to open-loop execution. To address these challenges, we propose Affordance-Graphed Task Worlds (AGT-World), a unified framework that autonomously constructs interactive simulated environments and corresponding robot ta...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:23:45+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12065v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12063v1] VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Yanjiang Guo, Tony Lee, Lucy Xiaoyang Shi, Jianyu Chen, Percy Liang, Chelsea Finn
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.RO</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        The goal of this paper is to improve the performance and reliability of vision-language-action (VLA) models through iterative online interaction. Since collecting policy rollouts in the real world is expensive, we investigate whether a learned simulator-specifically, an action-conditioned video generation model-can be used to generate additional rollout data. Unfortunately, existing world models lack the physical fidelity necessary for policy improvement: they are predominantly trained on demons...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:21:47+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12063v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12062v1] HoloBrain-0 Technical Report</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Xuewu Lin, Tianwei Lin, Yun Du, Hongyu Xie, Yiwei Jin, Jiawei Li, Shijie Wu, Qingze Wang, Mengdi Li, Mengao Zhao, Ziang Li, Chaodong Huang, Hongzhe Bi, Lichao Huang, Zhizhong Su
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.RO</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        In this work, we introduce HoloBrain-0, a comprehensive Vision-Language-Action (VLA) framework that bridges the gap between foundation model research and reliable real-world robot deployment. The core of our system is a novel VLA architecture that explicitly incorporates robot embodiment priors, including multi-view camera parameters and kinematic descriptions (URDF), to enhance 3D spatial reasoning and support diverse embodiments. We validate this design through a scalable ``pre-train then post...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:21:04+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12062v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12056v1] LawThinker: A Deep Research Legal Agent in Dynamic Environments</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Xinyu Yang, Chenlong Deng, Tongyu Wen, Binyu Xie, Zhicheng Dou
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Legal reasoning requires not only correct outcomes but also procedurally compliant reasoning processes. However, existing methods lack mechanisms to verify intermediate reasoning steps, allowing errors such as inapplicable statute citations to propagate undetected through the reasoning chain. To address this, we propose LawThinker, an autonomous legal research agent that adopts an Explore-Verify-Memorize strategy for dynamic judicial environments. The core idea is to enforce verification as an a...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:19:11+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12056v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12055v1] Multi UAVs Preflight Planning in a Shared and Dynamic Airspace</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Amath Sow, Mauricio Rodriguez Cesen, Fabiola Martins Campos de Oliveira, Mariusz Wzorek, Daniel de Leng, Mattias Tiger, Fredrik Heintz, Christian Esteve Rothenberg
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.MA</span>
                        
                        <span class="category">cs.RO</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Preflight planning for large-scale Unmanned Aerial Vehicle (UAV) fleets in dynamic, shared airspace presents significant challenges, including temporal No-Fly Zones (NFZs), heterogeneous vehicle profiles, and strict delivery deadlines. While Multi-Agent Path Finding (MAPF) provides a formal framework, existing methods often lack the scalability and flexibility required for real-world Unmanned Traffic Management (UTM). We propose DTAPP-IICR: a Delivery-Time Aware Prioritized Planning method with...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:18:46+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12055v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
            
        </div>
        
        <footer>
            <p>Áî± <a href="https://github.com/your-repo/auto-researcher">AutoResearcher</a> Ëá™Âä®ÁîüÊàê</p>
        </footer>
    </div>
</body>
</html>