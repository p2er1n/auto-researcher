<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ÂÖ∑Ë∫´Êô∫ËÉΩ &amp; VLA Â≠¶ÊúØËÆ∫Êñá</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
        }
        
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px 20px;
            text-align: center;
            border-radius: 8px;
            margin-bottom: 30px;
        }
        
        header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }
        
        header p {
            font-size: 1.1em;
            opacity: 0.9;
        }
        
        .meta {
            margin-top: 15px;
            font-size: 0.9em;
            opacity: 0.8;
        }
        
        .config-info {
            background: white;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        .config-info h3 {
            color: #333;
            margin-bottom: 15px;
            font-size: 1.1em;
        }
        
        .config-info .source-list {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin-bottom: 15px;
        }
        
        .config-info .source-tag {
            background: #667eea;
            color: white;
            padding: 4px 12px;
            border-radius: 15px;
            font-size: 0.85em;
        }
        
        .config-info .filter-list {
            color: #666;
            font-size: 0.9em;
        }
        
        .config-info .filter-list strong {
            color: #333;
        }
        
        .config-info .source-stats {
            margin-top: 10px;
            padding-top: 10px;
            border-top: 1px solid #eee;
        }
        
        .config-info .source-stats strong {
            color: #333;
        }
        
        .config-info .stat-tag {
            background: #e3f2fd;
            color: #1565c0;
            padding: 3px 10px;
            border-radius: 12px;
            font-size: 0.85em;
            margin-right: 8px;
            display: inline-block;
            margin-top: 5px;
        }
        
        .items {
            display: grid;
            gap: 20px;
        }
        
        .item {
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            transition: transform 0.2s, box-shadow 0.2s;
        }
        
        .item:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
        }
        
        .item h2 {
            font-size: 1.4em;
            color: #667eea;
            margin-bottom: 10px;
        }
        
        .item .authors {
            color: #666;
            font-size: 0.9em;
            margin-bottom: 10px;
        }
        
        .item .content {
            color: #555;
            margin: 10px 0;
        }
        
        .item .footer {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-top: 15px;
            padding-top: 15px;
            border-top: 1px solid #eee;
            font-size: 0.85em;
            color: #888;
            flex-wrap: wrap;
            gap: 10px;
        }
        
        .item .source-info {
            display: flex;
            gap: 8px;
            align-items: center;
            flex-wrap: wrap;
        }
        
        .item .source-type {
            background: #667eea;
            color: white;
            padding: 3px 10px;
            border-radius: 12px;
            font-size: 0.8em;
        }
        
        .item .venue {
            background: #e8f5e9;
            color: #2e7d32;
            padding: 3px 10px;
            border-radius: 12px;
            font-size: 0.8em;
        }
        
        .item .categories {
            display: flex;
            gap: 5px;
            flex-wrap: wrap;
        }
        
        .item .category {
            background: #e3f2fd;
            color: #1565c0;
            padding: 2px 8px;
            border-radius: 4px;
            font-size: 0.8em;
        }
        
        .item .date {
            color: #666;
        }
        
        .item a {
            color: #667eea;
            text-decoration: none;
        }
        
        .item a:hover {
            text-decoration: underline;
        }
        
        .btn-pdf {
            display: inline-block;
            background: #d32f2f;
            color: white;
            padding: 5px 12px;
            border-radius: 4px;
            font-size: 0.85em;
        }
        
        .btn-pdf:hover {
            background: #b71c1c;
            text-decoration: none;
        }
        
        footer {
            text-align: center;
            padding: 30px;
            color: #888;
            font-size: 0.9em;
        }
        
        .empty {
            text-align: center;
            padding: 60px 20px;
            color: #888;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>ÂÖ∑Ë∫´Êô∫ËÉΩ &amp; VLA Â≠¶ÊúØËÆ∫Êñá</h1>
            <p>Ëá™Âä®ËøΩË∏™ CVPR, NeurIPS, ICLR, CoRL, RSS Á≠âÈ°∂‰ºöÊúÄÊñ∞ÂÖ∑Ë∫´Êô∫ËÉΩ„ÄÅVLA„ÄÅÊú∫Âô®‰∫∫Áõ∏ÂÖ≥ËÆ∫Êñá</p>
            <div class="meta">
                ÁîüÊàêÊó∂Èó¥: 2026-02-14 07:14:18
            </div>
        </header>
        
        
        <div class="config-info">
            <h3>üìã ÈÖçÁΩÆ‰ø°ÊÅØ</h3>
            <div class="source-list">
                <strong>Êï∞ÊçÆÊ∫ê:</strong>
                
                    
                    
                        
                        <span class="source-tag">DBLP</span>
                        
                        
                    
                        
                        
                    
                        
                        <span class="source-tag">arXiv</span>
                        
                        
                    
                        
                        
                    
                
            </div>
            
            
            
            <div class="source-stats">
                <strong>üìä Êù•Ê∫êÁªüËÆ°:</strong>
                
                <span class="stat-tag">arXiv Library: 4 ÁØá</span>
                
            </div>
            
            
            <div class="filter-list">
                <strong>ÂÖ≥ÈîÆËØçËøáÊª§:</strong>
                
                    
                        
                            VLA, Vision-Language-Action, Vision Language Action, embodied, embodied intelligence, ÂÖ∑Ë∫´Êô∫ËÉΩ, robot, robotics, robotic, Êú∫Âô®‰∫∫, manipulation, grasping,  dexterous manipulation, navigation, visual navigation, language navigation, humanoid, android, ‰ªø‰∫∫Êú∫Âô®‰∫∫, sim-to-real, domain randomization, sim2real, reinforcement learning, RL, Âº∫ÂåñÂ≠¶‰π†, imitation learning, behavior cloning, Ê®°‰ªøÂ≠¶‰π†, LLM robot, large language model robot, ËØ≠Ë®ÄÊ®°ÂûãÊú∫Âô®‰∫∫, multimodal, multimodal learning, foundation model, visual foundation model, scene understanding, 3D understanding, object detection, object recognition, semantic understanding, scene graph
                        
                    
                        
                    
                        
                    
                
            </div>
        </div>
        
        <div class="items">
            
                
                
                <div class="item">
                    <h2>[2602.12281v1] Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-La...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Jacky Kwok, Xilun Zhang, Mengdi Xu, Yuejiang Liu, Azalia Mirhoseini, Chelsea Finn, Marco Pavone
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.RO</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">eess.SY</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the &#34;intention-action gap.&#39;&#39; We first characterize the test-time scaling law for embodied instruction following and demonstrate that...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:59:59+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12281v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12280v1] Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Huai-Hsun Cheng, Siang-Ling Zhang, Yu-Lun Liu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CV</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, a novel vector sketching task where a single sketch undergoes a dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise, a generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the &#34;dual-constraint&#34;:...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:59:54+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12280v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12279v1] UniT: Unified Multimodal Chain-of-Thought Test-time Scaling</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Leon Liangyu Chen, Haoyu Ma, Zhipeng Fan, Ziqi Huang, Animesh Sinha, Xiaoliang Dai, Jialiang Wang, Zecheng He, Jianwei Yang, Chunyuan Li, Junzhe Sun, Chu Wang, Serena Yeung-Levy, Felix Juefei-Xu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CV</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.LG</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional i...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:59:49+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12279v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12268v1] CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agen...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Zhen Zhang, Kaiqiang Song, Xun Wang, Yebowen Hu, Weixiang Yan, Chenyang Zhao, Henry Peng Zou, Haoyun Deng, Sathish Reddy Indurthi, Shujian Liu, Simin Ma, Xiaoyang Wang, Xin Eric Wang, Song Wang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        AI agents are increasingly used to solve real-world tasks by reasoning over multi-turn user interactions and invoking external tools. However, applying reinforcement learning to such settings remains difficult: realistic objectives often lack verifiable rewards and instead emphasize open-ended behaviors; moreover, RL for multi-turn, multi-step agentic tool use is still underexplored; and building and maintaining executable tool environments is costly, limiting scale and coverage. We propose CM2,...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:55:09+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12268v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
            
        </div>
        
        <footer>
            <p>Áî± <a href="https://github.com/your-repo/auto-researcher">AutoResearcher</a> Ëá™Âä®ÁîüÊàê</p>
        </footer>
    </div>
</body>
</html>