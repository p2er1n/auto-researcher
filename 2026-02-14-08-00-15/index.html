<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ÂÖ∑Ë∫´Êô∫ËÉΩ &amp; VLA Â≠¶ÊúØËÆ∫Êñá</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
        }
        
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px 20px;
            text-align: center;
            border-radius: 8px;
            margin-bottom: 30px;
        }
        
        header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }
        
        header p {
            font-size: 1.1em;
            opacity: 0.9;
        }
        
        .meta {
            margin-top: 15px;
            font-size: 0.9em;
            opacity: 0.8;
        }
        
        .config-info {
            background: white;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        .config-info h3 {
            color: #333;
            margin-bottom: 15px;
            font-size: 1.1em;
        }
        
        .config-info .source-list {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin-bottom: 15px;
        }
        
        .config-info .source-tag {
            background: #667eea;
            color: white;
            padding: 4px 12px;
            border-radius: 15px;
            font-size: 0.85em;
        }
        
        .config-info .filter-list {
            color: #666;
            font-size: 0.9em;
        }
        
        .config-info .filter-list strong {
            color: #333;
        }
        
        .config-info .source-stats {
            margin-top: 10px;
            padding-top: 10px;
            border-top: 1px solid #eee;
        }
        
        .config-info .source-stats strong {
            color: #333;
        }
        
        .config-info .stat-tag {
            background: #e3f2fd;
            color: #1565c0;
            padding: 3px 10px;
            border-radius: 12px;
            font-size: 0.85em;
            margin-right: 8px;
            display: inline-block;
            margin-top: 5px;
        }
        
        .items {
            display: grid;
            gap: 20px;
        }
        
        .item {
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            transition: transform 0.2s, box-shadow 0.2s;
        }
        
        .item:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
        }
        
        .item h2 {
            font-size: 1.4em;
            color: #667eea;
            margin-bottom: 10px;
        }
        
        .item .authors {
            color: #666;
            font-size: 0.9em;
            margin-bottom: 10px;
        }
        
        .item .content {
            color: #555;
            margin: 10px 0;
        }
        
        .item .footer {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-top: 15px;
            padding-top: 15px;
            border-top: 1px solid #eee;
            font-size: 0.85em;
            color: #888;
            flex-wrap: wrap;
            gap: 10px;
        }
        
        .item .source-info {
            display: flex;
            gap: 8px;
            align-items: center;
            flex-wrap: wrap;
        }
        
        .item .source-type {
            background: #667eea;
            color: white;
            padding: 3px 10px;
            border-radius: 12px;
            font-size: 0.8em;
        }
        
        .item .venue {
            background: #e8f5e9;
            color: #2e7d32;
            padding: 3px 10px;
            border-radius: 12px;
            font-size: 0.8em;
        }
        
        .item .categories {
            display: flex;
            gap: 5px;
            flex-wrap: wrap;
        }
        
        .item .category {
            background: #e3f2fd;
            color: #1565c0;
            padding: 2px 8px;
            border-radius: 4px;
            font-size: 0.8em;
        }
        
        .item .date {
            color: #666;
        }
        
        .item a {
            color: #667eea;
            text-decoration: none;
        }
        
        .item a:hover {
            text-decoration: underline;
        }
        
        .btn-pdf {
            display: inline-block;
            background: #d32f2f;
            color: white;
            padding: 5px 12px;
            border-radius: 4px;
            font-size: 0.85em;
        }
        
        .btn-pdf:hover {
            background: #b71c1c;
            text-decoration: none;
        }
        
        footer {
            text-align: center;
            padding: 30px;
            color: #888;
            font-size: 0.9em;
        }
        
        .empty {
            text-align: center;
            padding: 60px 20px;
            color: #888;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>ÂÖ∑Ë∫´Êô∫ËÉΩ &amp; VLA Â≠¶ÊúØËÆ∫Êñá</h1>
            <p>Ëá™Âä®ËøΩË∏™ CVPR, NeurIPS, ICLR, CoRL, RSS Á≠âÈ°∂‰ºöÊúÄÊñ∞ÂÖ∑Ë∫´Êô∫ËÉΩ„ÄÅVLA„ÄÅÊú∫Âô®‰∫∫Áõ∏ÂÖ≥ËÆ∫Êñá</p>
            <div class="meta">
                ÁîüÊàêÊó∂Èó¥: 2026-02-14 08:00:15
            </div>
        </header>
        
        
        <div class="config-info">
            <h3>üìã ÈÖçÁΩÆ‰ø°ÊÅØ</h3>
            <div class="source-list">
                <strong>Êï∞ÊçÆÊ∫ê:</strong>
                
                    
                    
                        
                        <span class="source-tag">DBLP</span>
                        
                        
                    
                        
                        
                    
                        
                        <span class="source-tag">arXiv</span>
                        
                        
                    
                        
                        
                    
                
            </div>
            
            
            
            <div class="source-stats">
                <strong>üìä Êù•Ê∫êÁªüËÆ°:</strong>
                
                <span class="stat-tag">DBLP RSS: 3 ÁØá</span>
                
                <span class="stat-tag">DBLP API: 98 ÁØá</span>
                
                <span class="stat-tag">arXiv Library: 61 ÁØá</span>
                
            </div>
            
            
            <div class="filter-list">
                <strong>ÂÖ≥ÈîÆËØçËøáÊª§:</strong>
                
                    
                        
                    
                        
                            VLA, Vision-Language-Action, Vision Language Action, embodied, embodied intelligence, ÂÖ∑Ë∫´Êô∫ËÉΩ, robot, robotics, robotic, Êú∫Âô®‰∫∫, manipulation, grasping, dexterous manipulation, navigation, visual navigation, language navigation, humanoid, android, ‰ªø‰∫∫Êú∫Âô®‰∫∫, sim-to-real, domain randomization, sim2real, reinforcement learning, RL, Âº∫ÂåñÂ≠¶‰π†, imitation learning, behavior cloning, Ê®°‰ªøÂ≠¶‰π†, LLM robot, large language model robot, ËØ≠Ë®ÄÊ®°ÂûãÊú∫Âô®‰∫∫, multimodal, multimodal learning, foundation model, visual foundation model, scene understanding, 3D understanding, object detection, object recognition, semantic understanding, scene graph
                        
                    
                        
                    
                        
                    
                
            </div>
        </div>
        
        <div class="items">
            
                
                
                <div class="item">
                    <h2>Position: Language model developers should report train-test overlap.</h2>
                    
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ICML</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ICML 
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ICML</span>
                            
                        </div>
                        <span class="date"></span>
                        
                            
                            <a href="https://dblp.org/rec/conf/icml/ZhangK0LZBL25" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Position: Lifetime tuning is incompatible with continual reinforcement learning.</h2>
                    
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ICML</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ICML 
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ICML</span>
                            
                        </div>
                        <span class="date"></span>
                        
                            
                            <a href="https://dblp.org/rec/conf/icml/MesbahiPMTW025" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Position: Constants are Critical in Regret Bounds for Reinforcement Learning.</h2>
                    
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ICML</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ICML 
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ICML</span>
                            
                        </div>
                        <span class="date"></span>
                        
                            
                            <a href="https://dblp.org/rec/conf/icml/DragoMM25b" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Contin...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Hongyu Ding, Ziming Xu, Yudong Fang, You Wu, Zixuan Chen, Jieqi Shi, Jing Huo, Yifan Zhang 0004, Yang Gao 0001
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">CoRR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        CoRR (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://arxiv.org/abs/2510.19655" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Multimodal fusion and vision-language models: A survey for robot vision.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Xiaofeng Han, Shunpeng Chen, Zenghuang Fu, Zhe Feng, Lue Fan, Dong An 0002, Changwei Wang 0001, Li Guo 0004, Weiliang Meng, Xiaopeng Zhang 0001, Rongtao Xu, Shibiao Xu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">Inf. Fusion</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Inf. Fusion (2026)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">Inf. Fusion</span>
                            
                        </div>
                        <span class="date">2026</span>
                        
                            
                            <a href="https://doi.org/10.1016/j.inffus.2025.103652" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>A Hierarchical Vision-Language and Reinforcement Learning Framework for Robotic Task and Motion Plan...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Junnan Zhang, Chaoxu Mu, Xin Xu 0001, Lei Ren 0001
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IEEE Robotics Autom. Lett.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IEEE Robotics Autom. Lett. (2026)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IEEE Robotics Autom. Lett.</span>
                            
                        </div>
                        <span class="date">2026</span>
                        
                            
                            <a href="https://doi.org/10.1109/LRA.2025.3629984" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Situation classification of living environment by daily life support robot using pre-trained large-s...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Yoshiki Obinata, Kento Kawaharazuka, Naoaki Kanazawa, Naoya Yamaguchi, Naoto Tsukamoto, Iori Yanokura, Shingo Kitagawa, Kei Okada, Masayuki Inaba
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">Adv. Robotics</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Adv. Robotics (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">Adv. Robotics</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1080/01691864.2025.2487608" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>SuctionPrompt: Visual-Assisted Robotic Picking with a Suction Cup Using Vision-Language Models and F...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Tomohiro Motoda, Takahide Kitamura, Ryo Hanai, Yukiyasu Domae
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">J. Robotics Mechatronics</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        J. Robotics Mechatronics (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">J. Robotics Mechatronics</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.20965/jrm.2025.p0374" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>GaussianVLM: Scene-Centric 3D Vision-Language Models Using Language-Aligned Gaussian Splats for Embo...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Anna-Maria Halacheva, Jan-Nico Zaech, Xi Wang 0021, Danda Pani Paudel, Luc Van Gool
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IEEE Robotics Autom. Lett.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IEEE Robotics Autom. Lett. (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IEEE Robotics Autom. Lett.</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/LRA.2025.3623037" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>VLM-Social-Nav: Socially Aware Robot Navigation Through Scoring Using Vision-Language Models.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Daeun Song, Jing Liang 0006, Amirreza Payandeh, Amir Hossain Raj, Xuesu Xiao, Dinesh Manocha
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IEEE Robotics Autom. Lett.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IEEE Robotics Autom. Lett. (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IEEE Robotics Autom. Lett.</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/LRA.2024.3511409" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Mobile Robot Navigation Using Hand-Drawn Maps: A Vision Language Model Approach.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Aaron Hao Tan, Angus Fung, Haitong Wang, Goldie Nejat
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IEEE Robotics Autom. Lett.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IEEE Robotics Autom. Lett. (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IEEE Robotics Autom. Lett.</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/LRA.2025.3577456" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>TinyVLA: Toward Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Junjie Wen, Yichen Zhu 0001, Jinming Li, Minjie Zhu, Zhibin Tang, Kun Wu 0001, Zhiyuan Xu, Ning Liu 0007, Ran Cheng, Chaomin Shen 0001, Yaxin Peng, Feifei Feng, Jian Tang 0008
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IEEE Robotics Autom. Lett.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IEEE Robotics Autom. Lett. (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IEEE Robotics Autom. Lett.</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/LRA.2025.3544909" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>OVL-MAP: An Online Visual Language Map Approach for Vision-and-Language Navigation in Continuous Env...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Shuhuan Wen, Ziyuan Zhang, Yuxiang Sun 0002, Zhiwen Wang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IEEE Robotics Autom. Lett.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IEEE Robotics Autom. Lett. (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IEEE Robotics Autom. Lett.</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/LRA.2025.3540577" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>VLM-MSGraph: Vision Language Model-enabled Multi-hierarchical Scene Graph for robotic assembly.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Shufei Li, Zhijie Yan, Zuoxu Wang, Yiping Gao
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">Robotics Comput. Integr. Manuf.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Robotics Comput. Integr. Manuf. (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">Robotics Comput. Integr. Manuf.</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1016/j.rcim.2025.102978" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Large vision-language models enabled novel objects 6D pose estimation for human-robot collaboration.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Wanqing Xia, Hao Zheng, Weiliang Xu 0001, Xun Xu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">Robotics Comput. Integr. Manuf.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Robotics Comput. Integr. Manuf. (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">Robotics Comput. Integr. Manuf.</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1016/j.rcim.2025.103030" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>VLM See, Robot Do: Human Demo Video to Robot Action Plan via Vision Language Model.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Beichen Wang, Juexiao Zhang, Shuwen Dong, Irving Fang, Chen Feng 0002
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IROS</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IROS (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IROS</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/IROS60139.2025.11246682" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Agreeing to Interact in Human-Robot Interaction using Large Language Models and Vision Language Mode...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Kazuhiro Sasabuchi, Naoki Wake, Atsushi Kanehira, Jun Takamatsu, Katsushi Ikeuchi
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">RO-MAN</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        RO-MAN (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">RO-MAN</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/RO-MAN63969.2025.11217646" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Xiaofeng Han, Shunpeng Chen, Zenghuang Fu, Zhe Feng, Lue Fan, Dong An 0002, Changwei Wang 0001, Li Guo 0004, Weiliang Meng, Xiaopeng Zhang 0001, Rongtao Xu, Shibiao Xu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">CoRR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        CoRR (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://arxiv.org/abs/2504.02477" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Youngjin Hong, Houjian Yu, Mingen Li, Changhyun Choi
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">CoRR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        CoRR (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://arxiv.org/abs/2511.02239" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Enshen Zhou, Cheng Chi 0001, Yibo Li, Jingkun An, Jiayuan Zhang, Shanyu Rong, Yi Han, Yuheng Ji, Mengzhen Liu, Pengwei Wang 0004, Zhongyuan Wang 0006, Lu Sheng, Shanghang Zhang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">CoRR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        CoRR (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://arxiv.org/abs/2512.13660" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Maestro: Orchestrating Robotics Modules with Vision-Language Models for Zero-Shot Generalist Robots.</h2>
                    
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">CoRR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        CoRR (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://arxiv.org/abs/2511.00917" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Robotic environmental state recognition with pre-trained vision-language models and black-box optimi...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Kento Kawaharazuka, Yoshiki Obinata, Naoaki Kanazawa, Kei Okada, Masayuki Inaba
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">Adv. Robotics</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Adv. Robotics (2024)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">Adv. Robotics</span>
                            
                        </div>
                        <span class="date">2024</span>
                        
                            
                            <a href="https://doi.org/10.1080/01691864.2024.2366995" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Reflectance estimation for proximity sensing by vision-language models: utilizing distributional sem...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Masashi Osada, Gustavo Alfonso Garcia Ricardez, Yosuke Suzuki, Tadahiro Taniguchi
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">Adv. Robotics</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Adv. Robotics (2024)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">Adv. Robotics</span>
                            
                        </div>
                        <span class="date">2024</span>
                        
                            
                            <a href="https://doi.org/10.1080/01691864.2024.2393408" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Continuous Object State Recognition for Cooking Robots Using Pre-Trained Vision-Language Models and...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Kento Kawaharazuka, Naoaki Kanazawa, Yoshiki Obinata, Kei Okada, Masayuki Inaba
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IEEE Robotics Autom. Lett.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IEEE Robotics Autom. Lett. (2024)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IEEE Robotics Autom. Lett.</span>
                            
                        </div>
                        <span class="date">2024</span>
                        
                            
                            <a href="https://doi.org/10.1109/LRA.2024.3375257" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Safe-VLN: Collision Avoidance for Vision-and-Language Navigation of Autonomous Robots Operating in C...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Lu Yue, Dongliang Zhou, Liang Xie 0012, Feitian Zhang, Ye Yan 0001, Erwei Yin
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IEEE Robotics Autom. Lett.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IEEE Robotics Autom. Lett. (2024)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IEEE Robotics Autom. Lett.</span>
                            
                        </div>
                        <span class="date">2024</span>
                        
                            
                            <a href="https://doi.org/10.1109/LRA.2024.3387171" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>VLMPC: Vision-Language Model Predictive Control for Robotic Manipulation.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Wentao Zhao, Jiaming Chen 0001, Ziyu Meng, Donghui Mao, Ran Song 0001, Wei Zhang 0021
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">Robotics - Science and Systems</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Robotics - Science and Systems (2024)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">Robotics - Science and Systems</span>
                            
                        </div>
                        <span class="date">2024</span>
                        
                            
                            <a href="https://doi.org/10.15607/RSS.2024.XX.106" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Enhancing Robot Explanation Capabilities through Vision-Language Models: a Preliminary Study by Inte...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: David Sobr√≠n-Hidalgo, Miguel √Ångel Gonz√°lez Santamarta, √Ångel Manuel Guerrero-Higueras, Francisco Javier Rodr√≠guez-Lera, Vicente Matell√°n Olivera
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">CoRR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        CoRR (2024)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                        </div>
                        <span class="date">2024</span>
                        
                            
                            <a href="https://arxiv.org/abs/2404.09705" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>LADEV: A Language-Driven Testing and Evaluation Platform for Vision-Language-Action Models in Roboti...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Zhijie Wang 0014, Zhehua Zhou, Jiayang Song, Yuheng Huang 0004, Zhan Shu, Lei Ma 0003
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">CoRR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        CoRR (2024)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                        </div>
                        <span class="date">2024</span>
                        
                            
                            <a href="https://arxiv.org/abs/2410.05191" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>TalkWithMachines: Enhancing Human-Robot Interaction for Interpretable Industrial Robotics Through La...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Ammar N. Abbas, Csaba Beleznai
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">CoRR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        CoRR (2024)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                        </div>
                        <span class="date">2024</span>
                        
                            
                            <a href="https://arxiv.org/abs/2412.15462" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Ted Xiao, Harris Chan, Pierre Sermanet, Ayzaan Wahid, Anthony Brohan, Karol Hausman, Sergey Levine, Jonathan Tompson
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">Robotics - Science and Systems</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Robotics - Science and Systems (2023)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">Robotics - Science and Systems</span>
                            
                        </div>
                        <span class="date">2023</span>
                        
                            
                            <a href="https://doi.org/10.15607/RSS.2023.XIX.029" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>V2A - Vision to Action: Learning Robotic Arm Actions Based on Vision and Language.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Michal Nazarczuk, Krystian Mikolajczyk
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ACCV</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ACCV (2020)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ACCV</span>
                            
                        </div>
                        <span class="date">2020</span>
                        
                            
                            <a href="https://doi.org/10.1007/978-3-030-69535-4_44" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Alignment in vision-based syntactic language games for teams of robots using stochastic regular gram...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Dar√≠o Maravall, Jack Mario Mingo, Javier de Lope
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">Robotics Auton. Syst.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Robotics Auton. Syst. (2015)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">Robotics Auton. Syst.</span>
                            
                        </div>
                        <span class="date">2015</span>
                        
                            
                            <a href="https://doi.org/10.1016/j.robot.2014.09.013" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Three-dimensional-grounded vision-language framework for robotic task planning: Automated prompt syn...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Guoqin Tang, Qingxuan Jia, Gang Chen 0029, Zeyuan Huang, Zhipeng Yao, Ning Ji
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">Eng. Appl. Artif. Intell.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Eng. Appl. Artif. Intell. (2026)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">Eng. Appl. Artif. Intell.</span>
                            
                        </div>
                        <span class="date">2026</span>
                        
                            
                            <a href="https://doi.org/10.1016/j.engappai.2025.113268" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Multimodal fusion with vision-language-action models for robotic manipulation: A systematic review.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Muhayy Ud Din, Waseem Akram 0001, Lyes Saad Saoud, Jan Rosell, Irfan Hussain
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">Inf. Fusion</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Inf. Fusion (2026)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">Inf. Fusion</span>
                            
                        </div>
                        <span class="date">2026</span>
                        
                            
                            <a href="https://doi.org/10.1016/j.inffus.2025.104062" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Vision-Language Artificial Intelligence for Robotic-Based Monitoring: Concrete Defect Detection, Cla...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Farzad Azizi Zade, Arvin Ebrahimkhanlou
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">J. Comput. Civ. Eng.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        J. Comput. Civ. Eng. (2026)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">J. Comput. Civ. Eng.</span>
                            
                        </div>
                        <span class="date">2026</span>
                        
                            
                            <a href="https://doi.org/10.1061/jccee5.cpeng-7001" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>GRAD-NAV++: Vision-Language Model Enabled Visual Drone Navigation With Gaussian Radiance Fields and...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Qianzhong Chen, Naixiang Gao, Suning Huang, JunEn Low, Timothy Chen, Jiankai Sun, Mac Schwager
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IEEE Robotics Autom. Lett.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IEEE Robotics Autom. Lett. (2026)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IEEE Robotics Autom. Lett.</span>
                            
                        </div>
                        <span class="date">2026</span>
                        
                            
                            <a href="https://doi.org/10.1109/LRA.2025.3643290" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Curiosity-Driven Zero-Shot Object Navigation With Vision-Language Models.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Zhengyi Gu, Yuheng Zhou, Yuquan Xue, Hanzhang Wang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IEEE Robotics Autom. Lett.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IEEE Robotics Autom. Lett. (2026)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IEEE Robotics Autom. Lett.</span>
                            
                        </div>
                        <span class="date">2026</span>
                        
                            
                            <a href="https://doi.org/10.1109/LRA.2026.3655277" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>GAIA: Generating Task Instruction Aware Simulation Grounded in Real Contexts Using Vision-Language M...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Dogyu Ko, Chanyoung Yeo, Daeho Kim, Jaeho Kim, Hyoseok Hwang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IEEE Robotics Autom. Lett.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IEEE Robotics Autom. Lett. (2026)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IEEE Robotics Autom. Lett.</span>
                            
                        </div>
                        <span class="date">2026</span>
                        
                            
                            <a href="https://doi.org/10.1109/LRA.2025.3617738" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>PointVLA: Injecting the 3D World Into Vision-Language-Action Models.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Chengmeng Li, Junjie Wen, Yaxin Peng, Yan Peng 0001, Yichen Zhu 0001
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IEEE Robotics Autom. Lett.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IEEE Robotics Autom. Lett. (2026)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IEEE Robotics Autom. Lett.</span>
                            
                        </div>
                        <span class="date">2026</span>
                        
                            
                            <a href="https://doi.org/10.1109/LRA.2026.3653303" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Sim2Real Diffusion: Leveraging Foundation Vision Language Models for Adaptive Automated Driving.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Chinmay Vilas Samak, Tanmay Vilas Samak, Bing Li 0008, Venkat Krovi
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IEEE Robotics Autom. Lett.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IEEE Robotics Autom. Lett. (2026)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IEEE Robotics Autom. Lett.</span>
                            
                        </div>
                        <span class="date">2026</span>
                        
                            
                            <a href="https://doi.org/10.1109/LRA.2025.3632723" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>FourierPlace: A Vision-Language Localization Framework Based on Frequency Domain Representations.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Tianyi Shang, Zhenyu Li, Shuaishuai Lu, Pengjie Xu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IEEE Robotics Autom. Lett.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IEEE Robotics Autom. Lett. (2026)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IEEE Robotics Autom. Lett.</span>
                            
                        </div>
                        <span class="date">2026</span>
                        
                            
                            <a href="https://doi.org/10.1109/LRA.2025.3634885" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Co-NavGPT: Multirobot Cooperative Visual Semantic Navigation Using Vision Language Models.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Bangguo Yu, Qihao Yuan, Kailai Li 0001, Hamidreza Kasaei 0001, Ming Cao 0001
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IEEE Robotics Autom. Lett.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IEEE Robotics Autom. Lett. (2026)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IEEE Robotics Autom. Lett.</span>
                            
                        </div>
                        <span class="date">2026</span>
                        
                            
                            <a href="https://doi.org/10.1109/LRA.2025.3645650" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>From drawings to decisions: A hybrid vision-language framework for parsing 2D engineering drawings i...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Muhammad Tayyab Khan, Lequn Chen 0002, Zane Yong, Jun Ming Tan, Wenhe Feng, Seung Ki Moon
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">Robotics Comput. Integr. Manuf.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Robotics Comput. Integr. Manuf. (2026)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">Robotics Comput. Integr. Manuf.</span>
                            
                        </div>
                        <span class="date">2026</span>
                        
                            
                            <a href="https://doi.org/10.1016/j.rcim.2025.103186" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>VTLG: A vision-tactile-language grasp generation method oriented towards task.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Tong Li, Chengshun Yu, Yuhang Yan 0001, Di Song, Yuxin Shuai, Yifan Wang, Gang Chen 0029
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">Robotics Comput. Integr. Manuf.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Robotics Comput. Integr. Manuf. (2026)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">Robotics Comput. Integr. Manuf.</span>
                            
                        </div>
                        <span class="date">2026</span>
                        
                            
                            <a href="https://doi.org/10.1016/j.rcim.2025.103152" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>RoboReward: General-Purpose Vision-Language Reward Models for Robotics.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Tony Lee, Andrew Wagenmaker, Karl Pertsch, Percy Liang, Sergey Levine, Chelsea Finn
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">CoRR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        CoRR (2026)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                        </div>
                        <span class="date">2026</span>
                        
                            
                            <a href="https://arxiv.org/abs/2601.00675" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>From Perception to Symbolic Task Planning: Vision-Language Guided Human-Robot Collaborative Structur...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Yanyi Chen, Min Deng
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">CoRR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        CoRR (2026)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                        </div>
                        <span class="date">2026</span>
                        
                            
                            <a href="https://arxiv.org/abs/2601.00978" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>LaST0: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Zhuoyang Liu, Jiaming Liu 0003, Hao Chen 0011, Jiale Yu, Ziyu Guo, Chengkai Hou, Chenyang Gu, Xiangju Mi, Renrui Zhang, Kun Wu 0001, Zhengping Che, Jian Tang 0008, Pheng-Ann Heng, Shanghang Zhang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">CoRR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        CoRR (2026)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                        </div>
                        <span class="date">2026</span>
                        
                            
                            <a href="https://arxiv.org/abs/2601.05248" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Kento Kawaharazuka, Jihoon Oh, Jun Yamada, Ingmar Posner, Yuke Zhu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IEEE Access</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IEEE Access (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IEEE Access</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/ACCESS.2025.3609980" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>A human-robot interaction system for automated chemical experiments based on vision and natural lang...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Zhuang Yang, Yu Du 0001, Dong Liu 0011, Kesong Zhao, Ming Cong 0001
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">Eng. Appl. Artif. Intell.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Eng. Appl. Artif. Intell. (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">Eng. Appl. Artif. Intell.</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1016/j.engappai.2025.110226" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Generalized Robot Vision-Language Model via Linguistic Foreground-Aware Contrast.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Kangcheng Liu, Chaoqun Wang, Xiaodong Han, Yong-Jin Liu 0001, Baoquan Chen
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">Int. J. Comput. Vis.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Int. J. Comput. Vis. (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">Int. J. Comput. Vis.</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1007/s11263-024-02340-z" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Correction: Generalized Robot Vision-Language Model via Linguistic Foreground-Aware Contrast.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Kangcheng Liu, Chaoqun Wang, Xiaodong Han, Yong-Jin Liu 0001, Baoquan Chen
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">Int. J. Comput. Vis.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Int. J. Comput. Vis. (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">Int. J. Comput. Vis.</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1007/s11263-025-02383-w" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Vision language model-enhanced embodied intelligence for digital twin-assisted human-robot collabora...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Changchun Liu 0002, Dunbing Tang, Haihua Zhu, Zequn Zhang, Liping Wang 0017, Yi Zhang 0136
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">J. Ind. Inf. Integr.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        J. Ind. Inf. Integr. (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">J. Ind. Inf. Integr.</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1016/j.jii.2025.100943" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Zhijie Wang 0014, Zhehua Zhou, Jiayang Song, Yuheng Huang 0004, Zhan Shu, Lei Ma 0003
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">Proc. ACM Softw. Eng.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Proc. ACM Softw. Eng. (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">Proc. ACM Softw. Eng.</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1145/3729343" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Leveraging Vision-Language Models for Open-Vocabulary Instance Segmentation and Tracking.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Bastian P√§tzold, Jan Nogga, Sven Behnke
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IEEE Robotics Autom. Lett.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IEEE Robotics Autom. Lett. (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IEEE Robotics Autom. Lett.</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/LRA.2025.3606363" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>ASMA: An $\underline{\text{A}}$daptive $\underline{\text{S}}$afety $\underline{\text{M}}$argin $\und...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Sourav Sanyal, Kaushik Roy 0001
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IEEE Robotics Autom. Lett.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IEEE Robotics Autom. Lett. (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IEEE Robotics Autom. Lett.</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/LRA.2025.3592138" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>VL-TGS: Trajectory Generation and Selection Using Vision Language Models in Mapless Outdoor Environm...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Daeun Song, Jing Liang 0006, Xuesu Xiao, Dinesh Manocha
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IEEE Robotics Autom. Lett.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IEEE Robotics Autom. Lett. (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IEEE Robotics Autom. Lett.</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/LRA.2025.3559822" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Boosting Efficient Reinforcement Learning for Vision-and-Language Navigation With Open-Sourced LLM.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Jiawei Wang, Teng Wang, Wenzhe Cai, Lele Xu, Changyin Sun 0001
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IEEE Robotics Autom. Lett.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IEEE Robotics Autom. Lett. (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IEEE Robotics Autom. Lett.</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/LRA.2024.3511402" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>A Review of Advances in Large Language and Vision Models for Robotic Manipulation: Techniques, Integ...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Sajjad Hussain, Shwetangshu Biswas, Amandip Dutta, Md Saad, Almas Baimagambetov, Khizer Saeed, Nikolaos Polatidis
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">SN Comput. Sci.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        SN Comput. Sci. (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">SN Comput. Sci.</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1007/s42979-025-04119-6" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Integrating With Multimodal Information for Enhancing Robotic Grasping With Vision-Language Models.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Zhou Zhao, Dongyuan Zheng, Yizi Chen, Jing Luo 0005, Yanjun Wang, Panfeng Huang, Chenguang Yang 0001
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IEEE Trans Autom. Sci. Eng.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IEEE Trans Autom. Sci. Eng. (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IEEE Trans Autom. Sci. Eng.</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/TASE.2025.3550360" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Surfer: A World Model-Based Framework for Vision-Language Robot Manipulation.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Pengzhen Ren, Kaidong Zhang, Hetao Zheng, Zixuan Li, Yuhang Wen 0001, Fengda Zhu, Shikui Ma, Xiaodan Liang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IEEE Trans. Neural Networks Learn. Syst.</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IEEE Trans. Neural Networks Learn. Syst. (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IEEE Trans. Neural Networks Learn. Syst.</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/TNNLS.2025.3594117" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Task-Oriented Robotic Manipulation with Vision Language Models.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Nurhan Bulus Guran, Hanchi Ren, Jingjing Deng 0001, Xianghua Xie
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ACIVS</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ACIVS (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ACIVS</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1007/978-3-032-07343-3_49" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Robot Failure Recovery Using Vision-Language Models With Optimized Prompts.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Hongyi Chen, Yunchao Yao, Ruixuan Liu, Changliu Liu, Jeffrey Ichnowski
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ACC</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ACC (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ACC</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.23919/ACC63710.2025.11107751" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>A Unified Framework for Real-Time Failure Handling in Robotics Using Vision-Language Models, Reactiv...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Faseeh Ahmad, Hashim Ismail, Jonathan Styrud, Maj Stenmark, Volker Kr√ºger
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">CASE</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        CASE (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">CASE</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/CASE58245.2025.11164021" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>ToolNavigator: Dataset Generation for Small Tools Handling and Vision-Language Navigation in Constru...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Mahdi Bonyani, Maryam Soleymani, Obiora Odugu, Chao Wang 0046
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">CASE</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        CASE (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">CASE</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/CASE58245.2025.11163988" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Xiaoqi Li 0009, Jingyun Xu, Mingxu Zhang, Jiaming Liu 0003, Yan Shen 0035, Iaroslav Ponomarenko, Jiahui Xu, Liang Heng, Siyuan Huang 0004, Shanghang Zhang, Hao Dong 0003
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">CVPR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        CVPR (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">CVPR</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_Object-Centric_Prompt-Driven_Vision-Language-Action_Model_for_Robotic_Manipulation_CVPR_2025_paper.html" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>RoboGround: Robotic Manipulation with Grounded Vision-Language Priors.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Haifeng Huang 0001, Xinyi Chen, Yilun Chen, Hao Li 0009, Xiaoshen Han, Zehan Wang 0001, Tai Wang, Jiangmiao Pang, Zhou Zhao 0001
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">CVPR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        CVPR (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">CVPR</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Huang_RoboGround_Robotic_Manipulation_with_Grounded_Vision-Language_Priors_CVPR_2025_paper.html" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su 0001, Stan Birchfield
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">CVPR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        CVPR (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">CVPR</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models_CVPR_2025_paper.html" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>A Vision-Language Framework for Assistive Home Robotics.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Magnus B√∏gh-Larsen, Adam Gerstnerlund, Davide Ragogna, Haris Alagic, Ozan Gazi Y√ºcel, Sepideh Valiollahi, Suzy Choi, Shahab Heshmati-Alamdari, Chen Li 0009, Dimitrios Chrysostomou
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ECMR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ECMR (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ECMR</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/ECMR65884.2025.11163226" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Zhongyi Zhou, Yichen Zhu 0001, Minjie Zhu, Junjie Wen, Ning Liu 0007, Zhiyuan Xu, Weibin Meng, Yaxin Peng, Chaomin Shen 0001, Feifei Feng, Yi Xu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">EMNLP</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        EMNLP (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">EMNLP</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.18653/v1/2025.emnlp-main.273" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Shake-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Manipulations and Liquid M...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Muhamamd Haris Khan, Selamawit Asfaw, Dmitrii Iarchuk, Miguel Altamirano Cabrera, Luis Moreno 0007, Issatay Tokmurziyev, Dzmitry Tsetserukou
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">HRI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        HRI (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">HRI</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/HRI61500.2025.10973961" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Friction-Aware Safety Locomotion for Wheeled-Legged Robots Using Vision Language Models and Reinforc...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Bo Peng, Donghoon Baek, Qijie Wang, Jo√£o Ramos 0004
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">Humanoids</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Humanoids (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">Humanoids</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/Humanoids65713.2025.11203074" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>LLaRA: Supercharging Robot Learning Data for Vision-Language Policy.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Xiang Li 0109, Cristina Mata, Jongwoo Park 0003, Kumara Kahatapitiya, Yoo Sung Jang, Jinghuan Shang, Kanchana Ranasinghe, Ryan D. Burgert, Mu Cai, Yong Jae Lee, Michael S. Ryoo
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ICLR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ICLR (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ICLR</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://openreview.net/forum?id=iVxxgZlXh6" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>AHA: A Vision-Language-Model for Detecting and Reasoning Over Failures in Robotic Manipulation.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Jiafei Duan, Wilbert Pumacay, Nishanth Kumar, Yi Ru Wang, Shulin Tian, Wentao Yuan, Ranjay Krishna, Dieter Fox, Ajay Mandlekar, Yijie Guo
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ICLR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ICLR (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ICLR</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://openreview.net/forum?id=JVkdSi7Ekg" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>VLAS: Vision-Language-Action Model with Speech Instructions for Customized Robot Manipulation.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Wei Zhao, Pengxiang Ding, Min Zhang 0036, Zhefei Gong, Shuanghao Bai, Han Zhao 0008, Donglin Wang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ICLR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ICLR (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ICLR</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://openreview.net/forum?id=K4FAFNRpko" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>USER-VLM 360: Personalized Vision Language Models with User-aware Tuning for Social Human-Robot Inte...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Hamed Rahimi, Adil Bahaj, Mouad Abrini, Mahdi Khoramshahi, Mounir Ghogho, Mohamed Chetouani
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ICMI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ICMI (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ICMI</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1145/3716553.3750767" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>ELEMENTAL: Interactive Learning from Demonstrations and Vision-Language Models for Reward Design in...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Letian Chen, Nina Marie Moorman, Matthew Craig Gombolay
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ICML</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ICML (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ICML</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://proceedings.mlr.press/v267/chen25at.html" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Lucy Xiaoyang Shi, Brian Ichter, Michael Robert Equi, Liyiming Ke, Karl Pertsch, Quan Vuong, James Tanner, Anna Walling, Haohuan Wang, Niccolo Fusai, Adrian Li-Bell, Danny Driess, Lachy Groom, Sergey Levine, Chelsea Finn
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ICML</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ICML (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ICML</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://proceedings.mlr.press/v267/shi25d.html" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Commonsense Reasoning for Legged Robot Adaptation with Vision-Language Models.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Annie S. Chen, Alec M. Lessing, Andy Tang, Govind Chada, Laura Smith 0001, Sergey Levine, Chelsea Finn
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ICRA</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ICRA (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ICRA</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/ICRA55743.2025.11127234" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>VLM-GroNav: Robot Navigation Using Physically Grounded Vision-Language Models in Outdoor Environment...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Mohamed Elnoor, Kasun Weerakoon, Gershom Seneviratne, Ruiqi Xian, Tianrui Guan, Mohamed Khalid M. Jaffar, Vignesh Rajagopal, Dinesh Manocha
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ICRA</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ICRA (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ICRA</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/ICRA55743.2025.11128264" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Towards Generalizable Vision-Language Robotic Manipulation: A Benchmark and LLM-Guided 3D Policy.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Ricardo Garcia 0001, Shizhe Chen, Cordelia Schmid
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ICRA</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ICRA (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ICRA</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/ICRA55743.2025.11127315" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>OLiVia-Nav: An Online Lifelong Vision Language Approach for Mobile Robot Social Navigation.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Siddarth Narasimhan, Aaron Hao Tan, Daniel Choi, Goldie Nejat
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ICRA</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ICRA (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ICRA</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/ICRA55743.2025.11128004" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>IntelliRMS: A Robotic Manipulation System for Domain-Specific Tasks Using Vision and Language Founda...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Chandan Kumar Singh, Devesh Kumar, Vipul Sanap, Mayank Khandelwal, Rajesh Sinha
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ICRA</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ICRA (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ICRA</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/ICRA55743.2025.11127298" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>KALIE: Fine-Tuning Vision-Language Models for Open-World Manipulation Without Robot Data.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Grace Tang, Swetha Rajkumar, Yifei Zhou, Homer Rich Walke, Sergey Levine, Kuan Fang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ICRA</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ICRA (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ICRA</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/ICRA55743.2025.11128156" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>SARO: Space-Aware Robot System for Terrain Crossing via Vision-Language Model.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Shaoting Zhu, Derun Li, Linzhan Mou, Yong Liu, Ningyi Xu, Hang Zhao 0021
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ICRA</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ICRA (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ICRA</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/ICRA55743.2025.11128428" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Medical Vision Language Models as Policies for Robotic Surgery.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Akshay Muppidi, Martin Radfar
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">CAI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        CAI (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">CAI</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/CAI64502.2025.00094" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Vision-Language Guided Adaptive Robot Action Planning: Responding to Intermediate Results and Implic...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Weihao Cai, Yoshiki Mori, Nobutaka Shimada
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IROS</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IROS (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IROS</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/IROS60139.2025.11246314" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>SDA-LLM: Spatial DisAmbiguation via Multi-turn Vision-Language Dialogues for Robot Navigation.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Kuan-Lin Chen, Tzu-Ti Wei, Ming-Lun Lee, Li-Tzu Yeh, Elaine Kao, Yu-Chee Tseng, Jen-Jee Chen
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IROS</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IROS (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IROS</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/IROS60139.2025.11246115" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>VLIN-RL: A Unified Vision-Language Interpreter and Reinforcement Learning Motion Planner Framework f...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Zewu Jiang, Junnan Zhang, Ke Wang, Chenyi Si
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IROS</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IROS (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IROS</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/IROS60139.2025.11247258" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>DiffGen: Robot Demonstration Generation via Differentiable Physics Simulation, Differentiable Render...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Yang Jin, Jun Lv, Shuqiang Jiang, Cewu Lu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IROS</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IROS (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IROS</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/IROS60139.2025.11247245" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment M...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Xin Li 0110, Siyuan Huang 0004, Qiaojun Yu, Zhengkai Jiang 0001, Ce Hao, Yimeng Zhu, Hongsheng Li 0001, Peng Gao 0007, Cewu Lu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IROS</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IROS (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IROS</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/IROS60139.2025.11246904" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>RoboNurse-VLA: Robotic Scrub Nurse System based on Vision-Language-Action Model.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Shunlei Li, Jin Wang, Rui Dai, Wanyu Ma, Wing Yin Ng, Yingbai Hu, Zheng Li 0012
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IROS</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IROS (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IROS</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/IROS60139.2025.11246030" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>TRACE: A Self-Improving Framework for Robot Behavior Forecasting with Vision-Language Models.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Gokul Puthumanaillam, Paulo Padrao, Jose Fuentes, Pranay Thangeda, William E. Schafer, Jae Hyuk Song, Karan Jagdale, Leonardo Bobadilla, Melkior Ornik
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IROS</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IROS (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IROS</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/IROS60139.2025.11247578" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Helpful DoggyBot: Open-World Object Fetching using Legged Robots and Vision-Language Models.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Qi Wu, Zipeng Fu, Xuxin Cheng, Xiaolong Wang 0004, Chelsea Finn
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IROS</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IROS (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IROS</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/IROS60139.2025.11246368" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Sensing Differently: Unifying Vision, Language, Posture and Tactile in Robotic Perception.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Yanmin Zhou, Yiyang Jin, Rong Jiang 0003, Xin Li 0093, Hongrui Sang, Shuo Jiang, Zhipeng Wang 0006, Bin He 0003
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">IROS</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        IROS (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">IROS</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/IROS60139.2025.11247209" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Can DeepSeek Reason Like a Surgeon? An Empirical Evaluation for Vision-Language Understanding in Rob...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Boyi Ma, Yanguang Zhao, Jie Wang 0097, Guankun Wang, Kun Yuan 0004, Tong Chen, Long Bai 0008, Hongliang Ren 0001
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">Agentic AI/CREATE/Clinical MLLMs@MICCAI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Agentic AI/CREATE/Clinical MLLMs@MICCAI (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">Agentic AI/CREATE/Clinical MLLMs@MICCAI</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1007/978-3-032-06004-4_18" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>RoboSoft&amp;apos;25: The 1st International Workshop on Vision-Language in Soft Robot.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Ziyu Wei, Luting Wang 0001, Chen Gao 0005, Hongliang Huang, Jiaqi Liu 0006, Li Wen, Si Liu 0001
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">ACM Multimedia</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        ACM Multimedia (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">ACM Multimedia</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1145/3746027.3762107" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Poster: Radar-Enhanced Robotic Material Perception with Vision Language Models.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Hongyu Deng, Jiangyou Zhu, He Chen 0001
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">MobiCom</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        MobiCom (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">MobiCom</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1145/3680207.3765653" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>RoboFlamingo-Plus: Fusion of Depth and RGB Perception with Vision-Language Models for Enhanced Robot...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Xiaojian Li, Sheng Wang, Chao Chen, Hailong Wei, Yudong Shi, Hangjie Mo
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">RCAR</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        RCAR (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">RCAR</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/RCAR65431.2025.11139480" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Robot-Led Vision Language Model Wellbeing Assessment of Children.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Nida Itrat Abbasi, Fethiye Irmak Dogan, Guy Laban, Joanna Anderson, Tamsin Ford, Peter B. Jones, Hatice Gunes
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">RO-MAN</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        RO-MAN (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">RO-MAN</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/RO-MAN63969.2025.11217833" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>Transparent Social Navigation for Autonomous Mobile Robots Via Vision-Language Models.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Oluwadamilola Sotomi, Devika Kodi, Aliasghar Arab
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">RO-MAN</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        RO-MAN (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">RO-MAN</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/RO-MAN63969.2025.11217693" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>QwenGrasp: Human-Robot Interactive 6-DoF Target-Oriented Grasping with Large Vision-Language Model.</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Xinyu Chen, Jian Yang 0031, Qi Zhao 0012, Zonghan He, Haobin Yang, Yuhui Shi 0001
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">SMC</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        SMC (2025)
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">DBLP</span>
                            
                            <span class="venue">SMC</span>
                            
                        </div>
                        <span class="date">2025</span>
                        
                            
                            <a href="https://doi.org/10.1109/SMC58881.2025.11343173" target="_blank">Êü•ÁúãÂéüÊñá ‚Üí</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12281v1] Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-La...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Jacky Kwok, Xilun Zhang, Mengdi Xu, Yuejiang Liu, Azalia Mirhoseini, Chelsea Finn, Marco Pavone
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.RO</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">eess.SY</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the &#34;intention-action gap.&#39;&#39; We first characterize the test-time scaling law for embodied instruction following and demonstrate that...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:59:59+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12281v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12280v1] Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Huai-Hsun Cheng, Siang-Ling Zhang, Yu-Lun Liu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CV</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, a novel vector sketching task where a single sketch undergoes a dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise, a generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the &#34;dual-constraint&#34;:...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:59:54+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12280v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12279v1] UniT: Unified Multimodal Chain-of-Thought Test-time Scaling</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Leon Liangyu Chen, Haoyu Ma, Zhipeng Fan, Ziqi Huang, Animesh Sinha, Xiaoliang Dai, Jialiang Wang, Zecheng He, Jianwei Yang, Chunyuan Li, Junzhe Sun, Chu Wang, Serena Yeung-Levy, Felix Juefei-Xu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CV</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.LG</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional i...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:59:49+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12279v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12268v1] CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agen...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Zhen Zhang, Kaiqiang Song, Xun Wang, Yebowen Hu, Weixiang Yan, Chenyang Zhao, Henry Peng Zou, Haoyun Deng, Sathish Reddy Indurthi, Shujian Liu, Simin Ma, Xiaoyang Wang, Xin Eric Wang, Song Wang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        AI agents are increasingly used to solve real-world tasks by reasoning over multi-turn user interactions and invoking external tools. However, applying reinforcement learning to such settings remains difficult: realistic objectives often lack verifiable rewards and instead emphasize open-ended behaviors; moreover, RL for multi-turn, multi-step agentic tool use is still underexplored; and building and maintaining executable tool environments is costly, limiting scale and coverage. We propose CM2,...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:55:09+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12268v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12253v1] Is Online Linear Optimization Sufficient for Strategic Robustness?</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Yang Cai, Haipeng Luo, Chen-Yu Wei, Weiqiang Zheng
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.GT</span>
                        
                        <span class="category">cs.LG</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        We consider bidding in repeated Bayesian first-price auctions. Bidding algorithms that achieve optimal regret have been extensively studied, but their strategic robustness to the seller&#39;s manipulation remains relatively underexplored. Bidding algorithms based on no-swap-regret algorithms achieve both desirable properties, but are suboptimal in terms of statistical and computational efficiency. In contrast, online gradient ascent is the only algorithm that achieves $O(\sqrt{TK})$ regret and strat...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:41:55+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12253v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12249v1] &#34;Sorry, I Didn&#39;t Catch That&#34;: How Speech Models Miss What Matters Most</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Kaitlyn Zhou, Martijn Bartelds, Federico Bianchi, James Zou
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.CL</span>
                        
                        <span class="category">cs.CY</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Despite speech recognition systems achieving low word error rates on standard benchmarks, they often fail on short, high-stakes utterances in real-world deployments. Here, we study this failure mode in a high-stakes task: the transcription of U.S. street names as spoken by U.S. participants. We evaluate 15 models from OpenAI, Deepgram, Google, and Microsoft on recordings from linguistically diverse U.S. speakers and find an average transcription error rate of 44%. We quantify the downstream impa...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:36:09+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12249v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12246v1] 6G Empowering Future Robotics: A Vision for Next-Generation Autonomous Systems</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Mona Ghassemian, Andr√©s Meseguer Valenzuela, Ana Garcia Armada, Dejan Vukobratovic, Periklis Chatzimisios, Kaspar Althoefer, Ranga Rao Venkatesha Prasad
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.NI</span>
                        
                        <span class="category">cs.RO</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        The convergence of robotics and next-generation communication is a critical driver of technological advancement. As the world transitions from 5G to 6G, the foundational capabilities of wireless networks are evolving to support increasingly complex and autonomous robotic systems. This paper examines the transformative impact of 6G on enhancing key robotics functionalities. It provides a systematic mapping of IMT-2030 key performance indicators to robotic functional blocks including sensing, perc...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:31:24+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12246v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12245v1] Intrinsic-Energy Joint Embedding Predictive Architectures Induce Quasimetric Spaces</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Anthony Kobanda, Waris Radji
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Joint-Embedding Predictive Architectures (JEPAs) aim to learn representations by predicting target embeddings from context embeddings, inducing a scalar compatibility energy in a latent space. In contrast, Quasimetric Reinforcement Learning (QRL) studies goal-conditioned control through directed distance values (cost-to-go) that support reaching goals under asymmetric dynamics. In this short article, we connect these viewpoints by restricting attention to a principled class of JEPA energy functi...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:30:27+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12245v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12244v1] Any House Any Task: Scalable Long-Horizon Planning for Abstract Human Tasks</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Zhihong Liu, Yang Li, Rengming Huang, Cewu Lu, Panpan Cai
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.RO</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Open world language conditioned task planning is crucial for robots operating in large-scale household environments. While many recent works attempt to address this problem using Large Language Models (LLMs) via prompting or training, a key challenge remains scalability. Performance often degrades rapidly with increasing environment size, plan length, instruction ambiguity, and constraint complexity. In this work, we propose Any House Any Task (AHAT), a household task planner optimized for long-...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:28:28+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12244v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12241v1] Moonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Manjunath Kudlur, Evan King, James Wang, Pete Warden
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CL</span>
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">cs.SD</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Latency-critical speech applications (e.g., live transcription, voice commands, and real-time translation) demand low time-to-first-token (TTFT) and high transcription accuracy, particularly on resource-constrained edge devices. Full-attention Transformer encoders remain a strong accuracy baseline for automatic speech recognition (ASR) because every frame can directly attend to every other frame, which resolves otherwise locally ambiguous acoustics using distant lexical context. However, this gl...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:20:45+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12241v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12237v1] Olmix: A Framework for Data Mixing Throughout LM Development</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Mayee F. Chen, Tyler Murray, David Heineman, Matt Jordan, Hannaneh Hajishirzi, Christopher R√©, Luca Soldaini, Kyle Lo
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.CL</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Data mixing -- determining the ratios of data from different domains -- is a first-order concern for training language models (LMs). While existing mixing methods show promise, they fall short when applied during real-world LM development. We present Olmix, a framework that addresses two such challenges. First, the configuration space for developing a mixing method is not well understood -- design choices across existing methods lack justification or consensus and overlook practical issues like...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:16:05+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12237v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12236v1] Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural Networks for Ne...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Anika Tabassum Meem, Muntasir Hossain Nadid, Md Zesun Ahmed Mia
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.NE</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.CV</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Neuromorphic vision systems based on spiking neural networks (SNNs) offer ultra-low-power perception for event-based and frame-based cameras, yet catastrophic forgetting remains a critical barrier to deployment in continually evolving environments. Existing continual learning methods, developed primarily for artificial neural networks, seldom jointly optimize accuracy and energy efficiency, with particularly limited exploration on event-based datasets. We propose an energy-aware spike budgeting...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:15:32+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12236v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12229v1] Diffusion Alignment Beyond KL: Variance Minimisation as Effective Policy Optimiser</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Zijing Ou, Jacob Si, Junyi Zhu, Ondrej Bohdal, Mete Ozay, Taha Ceritli, Yingzhen Li
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Diffusion alignment adapts pretrained diffusion models to sample from reward-tilted distributions along the denoising trajectory. This process naturally admits a Sequential Monte Carlo (SMC) interpretation, where the denoising model acts as a proposal and reward guidance induces importance weights. Motivated by this view, we introduce Variance Minimisation Policy Optimisation (VMPO), which formulates diffusion alignment as minimising the variance of log importance weights rather than directly op...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:06:03+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12229v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12224v1] Bandit Learning in Matching Markets with Interviews</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Amirmahdi Mirfakhar, Xuchuang Wang, Mengfan Xu, Hedyeh Beyhaghi, Mohammad Hajiesmaili
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.GT</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">econ.TH</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Two-sided matching markets rely on preferences from both sides, yet it is often impractical to evaluate preferences. Participants, therefore, conduct a limited number of interviews, which provide early, noisy impressions and shape final decisions. We study bandit learning in matching markets with interviews, modeling interviews as \textit{low-cost hints} that reveal partial preference information to both sides. Our framework departs from existing work by allowing firm-side uncertainty: firms, li...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T18:03:37+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12224v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12222v1] Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM T...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Miaosen Zhang, Yishan Liu, Shuxia Lin, Xu Yang, Qi Dai, Chong Luo, Weihao Jiang, Peng Hou, Anxiang Zeng, Xin Geng, Baining Guo
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.CV</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Supervised fine-tuning (SFT) is computationally efficient but often yields inferior generalization compared to reinforcement learning (RL). This gap is primarily driven by RL&#39;s use of on-policy data. We propose a framework to bridge this chasm by enabling On-Policy SFT. We first present \textbf{\textit{Distribution Discriminant Theory (DDT)}}, which explains and quantifies the alignment between data and the model-induced distribution. Leveraging DDT, we introduce two complementary techniques: (i...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:59:58+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12222v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12221v1] Best of Both Worlds: Multimodal Reasoning and Generation via Unified Discrete Flow Ma...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Onkar Susladkar, Tushar Prakash, Gayatri Deshmukh, Kiet A. Nguyen, Jiaxun Zhang, Adheesh Juvekar, Tianshu Bao, Lin Chai, Sparsh Mittal, Inderjit S Dhillon, Ismini Lourentzou
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CV</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        We propose UniDFlow, a unified discrete flow-matching framework for multimodal understanding, generation, and editing. It decouples understanding and generation via task-specific low-rank adapters, avoiding objective interference and representation entanglement, while a novel reference-based multimodal preference alignment optimizes relative outcomes under identical conditioning, improving faithfulness and controllability without large-scale retraining. UniDFlpw achieves SOTA performance across...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:59:08+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12221v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12218v1] The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Christian Intern√≤, Jumpei Yamaguchi, Loren Amdahl-Culleton, Markus Olhofer, David Klindt, Barbara Hammer
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Determining whether neural models internalize physical laws as world models, rather than exploiting statistical shortcuts, remains challenging, especially under out-of-distribution (OOD) shifts. Standard evaluations often test latent capability via downstream adaptation (e.g., fine-tuning or high-capacity probes), but such interventions can change the representations being measured and thus confound what was learned during self-supervised learning (SSL). We propose a non-invasive evaluation prot...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:56:07+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12218v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12215v1] LDA-1B: Scaling Latent Dynamics Action Model via Universal Embodied Data Ingestion</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Jiangran Lyu, Kai Liu, Xuheng Zhang, Haoran Liao, Yusen Feng, Wenxuan Zhu, Tingrui Shen, Jiayi Chen, Jiazhao Zhang, Yifei Dong, Wenbo Cui, Senmao Qi, Shuo Wang, Yixin Zheng, Mi Yan, Xuesong Shi, Haoran Li, Dongbin Zhao, Ming-Yu Liu, Zhizheng Zhang, Li Yi, Yizhou Wang, He Wang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.RO</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Recent robot foundation models largely rely on large-scale behavior cloning, which imitates expert actions but discards transferable dynamics knowledge embedded in heterogeneous embodied data. While the Unified World Model (UWM) formulation has the potential to leverage such diverse data, existing instantiations struggle to scale to foundation-level due to coarse data usage and fragmented datasets. We introduce LDA-1B, a robot foundation model that scales through universal embodied data ingestio...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:53:51+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12215v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12207v1] VIRENA: Virtual Arena for Research, Education, and Democratic Innovation</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Emma Hoes, K. Jonathan Klueser, Fabrizio Gilardi
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.HC</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.SI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Digital platforms shape how people communicate, deliberate, and form opinions. Studying these dynamics has become increasingly difficult due to restricted data access, ethical constraints on real-world experiments, and limitations of existing research tools. VIRENA (Virtual Arena) is a platform that enables controlled experimentation in realistic social media environments. Multiple participants interact simultaneously in realistic replicas of feed-based platforms (Instagram, Facebook, Reddit) an...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:46:52+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12207v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12205v1] DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation an...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Dianyi Wang, Ruihang Li, Feng Han, Chaofan Ma, Wei Song, Siyuan Wang, Yibin Wang, Yi Xin, Hongjian Liu, Zhixiong Zhang, Shengyuan Ding, Tianhang Wang, Zhenglin Cheng, Tao Lin, Cheng Jin, Kaicheng Yu, Jingjing Chen, Wenjie Wang, Zhongyu Wei, Jiaqi Wang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CV</span>
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., &gt;10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:44:24+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12205v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12203v1] ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from D...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Mathieu Sibue, Andres Mu√±oz Garza, Samuel Mensah, Pranav Shetty, Zhiqiang Ma, Xiaomo Liu, Manuela Veloso
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CL</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Enterprise documents, such as forms and reports, embed critical information for downstream applications like data archiving, automated workflows, and analytics. Although generalist Vision Language Models (VLMs) perform well on established document understanding benchmarks, their ability to conduct holistic, fine-grained structured extraction across diverse document types and flexible schemas is not well studied. Existing Key Entity Extraction (KEE), Relation Extraction (RE), and Visual Question...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:38:57+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12203v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12199v1] Sub--Riemannian boundary value problems for Optimal Geometric Locomotion</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Oliver Gross, Florine Hartwig, Martin Rumpf, Peter Schr√∂der
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.RO</span>
                        
                        <span class="category">math.NA</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        We propose a geometric model for optimal shape-change-induced motions of slender locomotors, e.g., snakes slithering on sand. In these scenarios, the motion of a body in world coordinates is completely determined by the sequence of shapes it assumes. Specifically, we formulate Lagrangian least-dissipation principles as boundary value problems whose solutions are given by sub-Riemannian geodesics. Notably, our geometric model accounts not only for the energy dissipated by the body&#39;s displacement...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:32:20+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12199v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12196v1] Visual Reasoning Benchmark: Evaluating Multimodal LLMs on Classroom-Authentic Visual...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Mohamed Huti, Alasdair Mackintosh, Amy Waldock, Dominic Andrews, Maxime Leli√®vre, Moritz Boos, Tobias Murray, Paul Atherton, Robin A. A. Ince, Oliver G. B. Garrod
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CL</span>
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        AI models have achieved state-of-the-art results in textual reasoning; however, their ability to reason over spatial and relational structures remains a critical bottleneck -- particularly in early-grade maths, which relies heavily on visuals. This paper introduces the visual reasoning benchmark (VRB), a novel dataset designed to evaluate Multimodal Large Language Models (MLLMs) on their ability to solve authentic visual problems from classrooms. This benchmark is built on a set of 701 questions...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:29:03+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12196v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12189v1] WaveFormer: Wavelet Embedding Transformer for Biomedical Signals</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Habib Irani, Bikram De, Vangelis Metsis
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Biomedical signal classification presents unique challenges due to long sequences, complex temporal dynamics, and multi-scale frequency patterns that are poorly captured by standard transformer architectures. We propose WaveFormer, a transformer architecture that integrates wavelet decomposition at two critical stages: embedding construction, where multi-channel Discrete Wavelet Transform (DWT) extracts frequency features to create tokens containing both time-domain and frequency-domain informat...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:20:43+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12189v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12187v1] SAGEO Arena: A Realistic Environment for Evaluating Search-Augmented Generative Engin...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Sunghwan Kim, Wooseok Jeong, Serin Kim, Sangam Lee, Dongha Lee
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.IR</span>
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Search-Augmented Generative Engines (SAGE) have emerged as a new paradigm for information access, bridging web-scale retrieval with generative capabilities to deliver synthesized answers. This shift has fundamentally reshaped how web content gains exposure online, giving rise to Search-Augmented Generative Engine Optimization (SAGEO), the practice of optimizing web documents to improve their visibility in AI-generated responses. Despite growing interest, no evaluation environment currently suppo...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:18:00+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12187v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12181v1] Convex Markov Games and Beyond: New Proof of Existence, Characterization and Learning...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Anas Barakat, Ioannis Panageas, Antonios Varvitsiotis
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.GT</span>
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">cs.MA</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Convex Markov Games (cMGs) were recently introduced as a broad class of multi-agent learning problems that generalize Markov games to settings where strategic agents optimize general utilities beyond additive rewards. While cMGs expand the modeling frontier, their theoretical foundations, particularly the structure of Nash equilibria (NE) and guarantees for learning algorithms, are not yet well understood. In this work, we address these gaps for an extension of cMGs, which we term General Utilit...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:11:20+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12181v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12180v1] How Sampling Shapes LLM Alignment: From One-Shot Optima to Iterative Dynamics</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Yurong Chen, Yu He, Michael I. Jordan, Fan Yao
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">cs.GT</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Standard methods for aligning large language models with human preferences learn from pairwise comparisons among sampled candidate responses and regularize toward a reference policy. Despite their effectiveness, the effects of sampling and reference choices are poorly understood theoretically. We investigate these effects through Identity Preference Optimization, a widely used preference alignment framework, and show that proper instance-dependent sampling can yield stronger ranking guarantees,...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:11:08+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12180v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12172v1] Pedagogically-Inspired Data Synthesis for Language Model Knowledge Distillation</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Bowei He, Yankai Chen, Xiaokun Zhang, Linghe Kong, Philip S. Yu, Xue Liu, Chen Ma
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.CL</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Knowledge distillation from Large Language Models (LLMs) to smaller models has emerged as a critical technique for deploying efficient AI systems. However, current methods for distillation via synthetic data lack pedagogical awareness, treating knowledge transfer as a one-off data synthesis and training task rather than a systematic learning process. In this paper, we propose a novel pedagogically-inspired framework for LLM knowledge distillation that draws from fundamental educational principle...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T17:00:36+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12172v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12170v1] Statistical Parsing for Logical Information Retrieval</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Greg Coppola
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        In previous work (Coppola, 2024) we introduced the Quantified Boolean Bayesian Network (QBBN), a logical graphical model that implements the forward fragment of natural deduction (Prawitz, 1965) as a probabilistic factor graph. That work left two gaps: no negation/backward reasoning, and no parser for natural language.
  This paper addresses both gaps across inference, semantics, and syntax. For inference, we extend the QBBN with NEG factors enforcing P(x) + P(neg x) = 1, enabling contrapositive...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:57:25+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12170v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12160v1] DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Xu Guo, Fulong Ye, Qichao Sun, Liyang Chen, Bingchuan Li, Pengze Zhang, Jiawei Liu, Songtao Zhao, Qian He, Xiangwang Hou
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CV</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni, a...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:41:52+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12160v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12159v1] 3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Wancai Zheng, Hao Chen, Xianlong Lu, Linlin Ou, Xinyi Yu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.RO</span>
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Object navigation is a core capability of embodied intelligence, enabling an agent to locate target objects in unknown environments. Recent advances in vision-language models (VLMs) have facilitated zero-shot object navigation (ZSON). However, existing methods often rely on scene abstractions that convert environments into semantic maps or textual representations, causing high-level decision making to be constrained by the accuracy of low-level perception. In this work, we present 3DGSNav, a nov...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:41:26+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12159v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12158v1] SafeNeuron: Neuron-Level Safety Alignment for Large Language Models</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Zhaoxin Wang, Jiaming Liang, Fengbin Zhu, Weixiang Zhao, Junfeng Fang, Jiayi Ji, Handing Wang, Tat-Seng Chua
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Large language models (LLMs) and multimodal LLMs are typically safety-aligned before release to prevent harmful content generation. However, recent studies show that safety behaviors are concentrated in a small subset of parameters, making alignment brittle and easily bypassed through neuron-level attacks. Moreover, most existing alignment methods operate at the behavioral level, offering limited control over the model&#39;s internal safety mechanisms. In this work, we propose SafeNeuron, a neuron-l...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:40:05+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12158v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12155v1] FAIL: Flow Matching Adversarial Imitation Learning for Image Generation</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Yeyao Ma, Chen Li, Xiaosong Zhang, Han Hu, Weidi Xie
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CV</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Post-training of flow matching models-aligning the output distribution with a high-quality target-is mathematically equivalent to imitation learning. While Supervised Fine-Tuning mimics expert demonstrations effectively, it cannot correct policy drift in unseen states. Preference optimization methods address this but require costly preference pairs or reward modeling. We propose Flow Matching Adversarial Imitation Learning (FAIL), which minimizes policy-expert divergence through adversarial trai...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:36:33+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12155v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12147v1] It&#39;s TIME: Towards the Next Generation of Time Series Forecasting Benchmarks</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Zhongzheng Qiao, Sheng Pan, Anni Wang, Viktoriya Zhukova, Yong Liu, Xudong Jiang, Qingsong Wen, Mingsheng Long, Ming Jin, Chenghao Liu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Time series foundation models (TSFMs) are revolutionizing the forecasting landscape from specific dataset modeling to generalizable task evaluation. However, we contend that existing benchmarks exhibit common limitations in four dimensions: constrained data composition dominated by reused legacy sources, compromised data integrity lacking rigorous quality assurance, misaligned task formulations detached from real-world contexts, and rigid analysis perspectives that obscure generalizable insights...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:31:01+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12147v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12146v1] Seq2Seq2Seq: Lossless Data Compression via Discrete Latent Transformers and Reinforce...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Mahdi Khodabandeh, Ghazal Shabani, Arash Yousefi Jordehi, Seyed Abolghasem Mirroshandel
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.CL</span>
                        
                        <span class="category">cs.IT</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Efficient lossless compression is essential for minimizing storage costs and transmission overhead while preserving data integrity. Traditional compression techniques, such as dictionary-based and statistical methods, often struggle to optimally exploit the structure and redundancy in complex data formats. Recent advancements in deep learning have opened new avenues for compression; however, many existing approaches depend on dense vector representations that obscure the underlying token structu...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:30:55+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12146v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12144v1] On the Adoption of AI Coding Agents in Open-source Android and iOS Development</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Muhammad Ahmad Khan, Hasnain Ali, Muneeb Rana, Muhammad Saqib Ilyas, Abdul Ali Bangash
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.SE</span>
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        AI coding agents are increasingly contributing to software development, yet their impact on mobile development has received little empirical attention. In this paper, we present the first category-level empirical study of agent-generated code in open-source mobile app projects. We analyzed PR acceptance behaviors across mobile platforms, agents, and task categories using 2,901 AI-authored pull requests (PRs) in 193 verified Android and iOS open-source GitHub repositories in the AIDev dataset. We...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:30:29+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12144v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12137v1] CitiLink-Minutes: A Multilayer Annotated Dataset of Municipal Meeting Minutes</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Ricardo Campos, Ana Filipa Pacheco, Ana Lu√≠sa Fernandes, In√™s Cantante, Rute Rebou√ßas, Lu√≠s Filipe Cunha, Jos√© Miguel Isidro, Jos√© Pedro Evans, Miguel Marques, Rodrigo Batista, Evelin Amorim, Al√≠pio Jorge, Nuno Guimar√£es, S√©rgio Nunes, Ant√≥nio Leal, Purifica√ß√£o Silvano
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CL</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        City councils play a crucial role in local governance, directly influencing citizens&#39; daily lives through decisions made during municipal meetings. These deliberations are formally documented in meeting minutes, which serve as official records of discussions, decisions, and voting outcomes. Despite their importance, municipal meeting records have received little attention in Information Retrieval (IR) and Natural Language Processing (NLP), largely due to the lack of annotated datasets, which ult...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:22:55+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12137v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12135v1] WavBench: Benchmarking Reasoning, Colloquialism, and Paralinguistics for End-to-End S...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Yangzhuo Li, Shengpeng Ji, Yifu Chen, Tianle Liang, Haorong Ying, Yule Wang, Junbo Li, Jun Fang, Zhou Zhao
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CL</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        With the rapid integration of advanced reasoning capabilities into spoken dialogue models, the field urgently demands benchmarks that transcend simple interactions to address real-world complexity. However, current evaluations predominantly adhere to text-generation standards, overlooking the unique audio-centric characteristics of paralinguistics and colloquialisms, alongside the cognitive depth required by modern agents. To bridge this gap, we introduce WavBench, a comprehensive benchmark desi...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:22:11+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12135v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12133v1] Neutral Prompts, Non-Neutral People: Quantifying Gender and Skin-Tone Bias in Gemini...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Roberto Balestri
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.CL</span>
                        
                        <span class="category">cs.CY</span>
                        
                        <span class="category">cs.HC</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        This study quantifies gender and skin-tone bias in two widely deployed commercial image generators - Gemini Flash 2.5 Image (NanoBanana) and GPT Image 1.5 - to test the assumption that neutral prompts yield demographically neutral outputs. We generated 3,200 photorealistic images using four semantically neutral prompts. The analysis employed a rigorous pipeline combining hybrid color normalization, facial landmark masking, and perceptually uniform skin tone quantification using the Monk (MST), P...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:21:03+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12133v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12125v1] Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Wenkai Yang, Weijie Liu, Ruobing Xie, Kai Yang, Saiyong Yang, Yankai Lin
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.CL</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        On-policy distillation (OPD), which aligns the student with the teacher&#39;s logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:14:29+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12125v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12124v1] Capability-Oriented Training Induced Alignment Risk</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Yujun Zhou, Yue Huang, Han Bao, Kehan Guo, Zhenwen Liang, Pin-Yu Chen, Tian Gao, Werner Geyer, Nuno Moniz, Nitesh V Chawla, Xiangliang Zhang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">cs.CL</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        While most AI alignment research focuses on preventing models from generating explicitly harmful content, a more subtle risk is emerging: capability-oriented training induced exploitation. We investigate whether language models, when trained with reinforcement learning (RL) in environments with implicit loopholes, will spontaneously learn to exploit these flaws to maximize their reward, even without any malicious intent in their training. To test this, we design a suite of four diverse &#34;vulnerab...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:13:14+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12124v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12123v1] Meta-Sel: Efficient Demonstration Selection for In-Context Learning via Supervised Me...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Xubin Wang, Weijia Jia
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.CL</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Demonstration selection is a practical bottleneck in in-context learning (ICL): under a tight prompt budget, accuracy can change substantially depending on which few-shot examples are included, yet selection must remain cheap enough to run per query over large candidate pools. We propose Meta-Sel, a lightweight supervised meta-learning approach for intent classification that learns a fast, interpretable scoring function for (candidate, query) pairs from labeled training data.
  Meta-Sel construc...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:11:29+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12123v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12120v1] Commencing-Student Enrolment Forecasting Under Data Sparsity with Time Series Foundat...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Jittarin Jetwiriyanon, Teo Susnjak, Surangika Ranathunga
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Many universities face increasing financial pressure and rely on accurate forecasts of commencing enrolments. However, enrolment forecasting in higher education is often data-sparse; annual series are short and affected by reporting changes and regime shifts. Popular classical approaches can be unreliable, as parameter estimation and model selection are unstable with short samples, and structural breaks degrade extrapolation. Recently, TSFMs have provided zero-shot priors, delivering strong gain...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:10:42+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12120v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12117v1] KAN-FIF: Spline-Parameterized Lightweight Physics-based Tropical Cyclone Estimation o...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Jiakang Shen, Qinghui Chen, Runtong Wang, Chenrui Xu, Jinglin Zhang, Cong Bai, Feng Zhang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Tropical cyclones (TC) are among the most destructive natural disasters, causing catastrophic damage to coastal regions through extreme winds, heavy rainfall, and storm surges. Timely monitoring of tropical cyclones is crucial for reducing loss of life and property, yet it is hindered by the computational inefficiency and high parameter counts of existing methods on resource-constrained edge devices. Current physics-guided models suffer from linear feature interactions that fail to capture high-...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:07:39+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12117v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12116v1] P-GenRM: Personalized Generative Reward Model with Test-time User-based Scaling</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Pinyi Zhang, Ting-En Lin, Yuchuan Wu, Jingyang Chen, Zongqi Wang, Hua Yang, Ze Xu, Fei Huang, Kai Zhang, Yongbin Li
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CL</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Personalized alignment of large language models seeks to adapt responses to individual user preferences, typically via reinforcement learning. A key challenge is obtaining accurate, user-specific reward signals in open-ended scenarios. Existing personalized reward models face two persistent limitations: (1) oversimplifying diverse, scenario-specific preferences into a small, fixed set of evaluation principles, and (2) struggling with generalization to new users with limited feedback. To this end...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:07:22+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12116v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12113v1] Stop Unnecessary Reflection: Training LRMs for Efficient Reasoning with Adaptive Refl...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Zewei Yu, Lirong Gao, Yuke Zhu, Bo Zheng, Sheng Guo, Haobo Wang, Junbo Zhao
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.CL</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks by employing test-time scaling. However, they often generate over-long chains-of-thought that, driven by substantial reflections such as repetitive self-questioning and circular reasoning, lead to high token consumption, substantial computational overhead, and increased latency without improving accuracy, particularly in smaller models. Our observation reveals that increasing problem complexity indu...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:04:00+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12113v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12112v1] Few-Shot Design Optimization by Exploiting Auxiliary Information</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Arjun Mani, Carl Vondrick, Richard Zemel
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Many real-world design problems involve optimizing an expensive black-box function $f(x)$, such as hardware design or drug discovery. Bayesian Optimization has emerged as a sample-efficient framework for this problem. However, the basic setting considered by these methods is simplified compared to real-world experimental setups, where experiments often generate a wealth of useful information. We introduce a new setting where an experiment generates high-dimensional auxiliary information $h(x)$ a...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:03:46+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12112v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12108v1] The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Xiaoyuan Liu, Tian Liang, Dongyang Ma, Deyu Zhou, Haitao Mi, Pinjia He, Yan Wang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        In the world of Harry Potter, when Dumbledore&#39;s mind is overburdened, he extracts memories into a Pensieve to be revisited later. In the world of AI, while we possess the Pensieve-mature databases and retrieval systems, our models inexplicably lack the &#34;wand&#34; to operate it. They remain like a Dumbledore without agency, passively accepting a manually engineered context as their entire memory. This work finally places the wand in the model&#39;s hand. We introduce StateLM, a new class of foundation mo...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T16:00:01+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12108v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12107v1] On the Complexity of Offline Reinforcement Learning with $Q^\star$-Approximation and...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Haolin Liu, Braham Snyder, Chen-Yu Wei
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">stat.ML</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        We study offline reinforcement learning under $Q^\star$-approximation and partial coverage, a setting that motivates practical algorithms such as Conservative $Q$-Learning (CQL; Kumar et al., 2020) but has received limited theoretical attention. Our work is inspired by the following open question: &#34;Are $Q^\star$-realizability and Bellman completeness sufficient for sample-efficient offline RL under partial coverage?&#34;
  We answer in the negative by establishing an information-theoretic lower boun...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:59:42+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12107v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12100v1] AssetFormer: Modular 3D Assets Generation with Autoregressive Transformer</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Lingting Zhu, Shengju Qian, Haidi Fan, Jiayu Dong, Zhenchao Jin, Siwei Zhou, Gen Dong, Xin Wang, Lequan Yu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CV</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        The digital industry demands high-quality, diverse modular 3D assets, especially for user-generated content~(UGC). In this work, we introduce AssetFormer, an autoregressive Transformer-based model designed to generate modular 3D assets from textual descriptions. Our pilot study leverages real-world modular assets collected from online platforms. AssetFormer tackles the challenge of creating assets composed of primitives that adhere to constrained design parameters for various applications. By in...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:55:21+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12100v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12099v1] GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: GigaBrain Team, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Hao Li, Jie Li, Jindi Lv, Jingyu Liu, Lv Feng, Mingming Yu, Peng Li, Qiuping Deng, Tianze Liu, Xinyu Zhou, Xinze Chen, Xiaofeng Wang, Yang Wang, Yifan Li, Yifei Nie, Yilong Li, Yukun Zhou, Yun Ye, Zhichao Liu, Zheng Zhu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CV</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Vision-language-action (VLA) models that directly predict multi-step action chunks from current observations face inherent limitations due to constrained scene understanding and weak future anticipation capabilities. In contrast, video world models pre-trained on web-scale video corpora exhibit robust spatiotemporal reasoning and accurate future prediction, making them a natural foundation for enhancing VLA learning. Therefore, we propose \textit{GigaBrain-0.5M*}, a VLA model trained via world m...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:55:19+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12099v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12096v1] Multi Graph Search for High-Dimensional Robot Motion Planning</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Itamar Mishani, Maxim Likhachev
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.RO</span>
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Efficient motion planning for high-dimensional robotic systems, such as manipulators and mobile manipulators, is critical for real-time operation and reliable deployment. Although advances in planning algorithms have enhanced scalability to high-dimensional state spaces, these improvements often come at the cost of generating unpredictable, inconsistent motions or requiring excessive computational resources and memory. In this work, we introduce Multi-Graph Search (MGS), a search-based motion pl...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:50:15+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12096v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12092v1] DeepSight: An All-in-One LM Safety Toolkit</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Bo Zhang, Jiaxuan Guo, Lijun Li, Dongrui Liu, Sujin Chen, Guanxu Chen, Zhijie Zheng, Qihao Lin, Lewen Yan, Chen Qian, Yijin Zhou, Yuyao Wu, Shaoxiong Guo, Tianyi Du, Jingyi Yang, Xuhao Hu, Ziqi Miao, Xiaoya Lu, Jing Shao, Xia Hu
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.CL</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.CR</span>
                        
                        <span class="category">cs.CV</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluation can only locate external behavioral risks but cannot figure out internal root causes. Meanwhile, safety diagnosis often drifts from concrete risk scenarios and remains at the explainable level. In t...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:43:14+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12092v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12089v1] Choose Your Agent: Tradeoffs in Adopting AI Advisors, Coaches, and Delegates in Multi...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Kehang Zhu, Lithium Thain, Vivian Tsai, James Wexler, Crystal Qian
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.GT</span>
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.HC</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        As AI usage becomes more prevalent in social contexts, understanding agent-user interaction is critical to designing systems that improve both individual and group outcomes. We present an online behavioral experiment (N = 243) in which participants play three multi-turn bargaining games in groups of three. Each game, presented in randomized order, grants \textit{access to} a single LLM assistance modality: proactive recommendations from an \textit{Advisor}, reactive feedback from a \textit{Coach...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:41:57+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12089v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12087v1] Geometry of Uncertainty: Learning Metric Spaces for Multimodal State Estimation in RL</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Alfredo Reichlin, Adriano Pacciarelli, Danica Kragic, Miguel Vasco
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.LG</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Estimating the state of an environment from high-dimensional, multimodal, and noisy observations is a fundamental challenge in reinforcement learning (RL). Traditional approaches rely on probabilistic models to account for the uncertainty, but often require explicit noise assumptions, in turn limiting generalization. In this work, we contribute a novel method to learn a structured latent representation, in which distances between states directly correlate with the minimum number of actions requi...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:41:20+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12087v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12074v1] RF-Modulated Adaptive Communication Improves Multi-Agent Robotic Exploration</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Lorin Achey, Breanne Crockett, Christoffer Heckman, Bradley Hayes
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.RO</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Reliable coordination and efficient communication are critical challenges for multi-agent robotic exploration of environments where communication is limited. This work introduces Adaptive-RF Transmission (ART), a novel communication-aware planning algorithm that dynamically modulates transmission location based on signal strength and data payload size, enabling heterogeneous robot teams to share information efficiently without unnecessary backtracking. We further explore an extension to this app...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:33:17+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12074v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12065v1] Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied L...</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Xiang Liu, Sen Cui, Guocai Yao, Zhong Cao, Jingheng Ma, Min Zhang, Changshui Zhang
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.RO</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Training robotic policies directly in the real world is expensive and unscalable. Although generative simulation enables large-scale data synthesis, current approaches often fail to generate logically coherent long-horizon tasks and struggle with dynamic physical uncertainties due to open-loop execution. To address these challenges, we propose Affordance-Graphed Task Worlds (AGT-World), a unified framework that autonomously constructs interactive simulated environments and corresponding robot ta...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:23:45+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12065v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12063v1] VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Yanjiang Guo, Tony Lee, Lucy Xiaoyang Shi, Jianyu Chen, Percy Liang, Chelsea Finn
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.RO</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        The goal of this paper is to improve the performance and reliability of vision-language-action (VLA) models through iterative online interaction. Since collecting policy rollouts in the real world is expensive, we investigate whether a learned simulator-specifically, an action-conditioned video generation model-can be used to generate additional rollout data. Unfortunately, existing world models lack the physical fidelity necessary for policy improvement: they are predominantly trained on demons...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:21:47+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12063v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12062v1] HoloBrain-0 Technical Report</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Xuewu Lin, Tianwei Lin, Yun Du, Hongyu Xie, Yiwei Jin, Jiawei Li, Shijie Wu, Qingze Wang, Mengdi Li, Mengao Zhao, Ziang Li, Chaodong Huang, Hongzhe Bi, Lichao Huang, Zhizhong Su
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.RO</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        In this work, we introduce HoloBrain-0, a comprehensive Vision-Language-Action (VLA) framework that bridges the gap between foundation model research and reliable real-world robot deployment. The core of our system is a novel VLA architecture that explicitly incorporates robot embodiment priors, including multi-view camera parameters and kinematic descriptions (URDF), to enhance 3D spatial reasoning and support diverse embodiments. We validate this design through a scalable ``pre-train then post...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:21:04+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12062v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12056v1] LawThinker: A Deep Research Legal Agent in Dynamic Environments</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Xinyu Yang, Chenlong Deng, Tongyu Wen, Binyu Xie, Zhicheng Dou
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Legal reasoning requires not only correct outcomes but also procedurally compliant reasoning processes. However, existing methods lack mechanisms to verify intermediate reasoning steps, allowing errors such as inapplicable statute citations to propagate undetected through the reasoning chain. To address this, we propose LawThinker, an autonomous legal research agent that adopts an Explore-Verify-Memorize strategy for dynamic judicial environments. The core idea is to enforce verification as an a...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:19:11+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12056v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
                
                <div class="item">
                    <h2>[2602.12055v1] Multi UAVs Preflight Planning in a Shared and Dynamic Airspace</h2>
                    
                    
                    <div class="authors">
                        ‰ΩúËÄÖ: Amath Sow, Mauricio Rodriguez Cesen, Fabiola Martins Campos de Oliveira, Mariusz Wzorek, Daniel de Leng, Mattias Tiger, Fredrik Heintz, Christian Esteve Rothenberg
                    </div>
                    
                    
                    
                    <div class="categories">
                        
                        <span class="category">cs.AI</span>
                        
                        <span class="category">cs.MA</span>
                        
                        <span class="category">cs.RO</span>
                        
                    </div>
                    
                    
                    <div class="content">
                        Preflight planning for large-scale Unmanned Aerial Vehicle (UAV) fleets in dynamic, shared airspace presents significant challenges, including temporal No-Fly Zones (NFZs), heterogeneous vehicle profiles, and strict delivery deadlines. While Multi-Agent Path Finding (MAPF) provides a formal framework, existing methods often lack the scalability and flexibility required for real-world Unmanned Traffic Management (UTM). We propose DTAPP-IICR: a Delivery-Time Aware Prioritized Planning method with...
                    </div>
                    <div class="footer">
                        <div class="source-info">
                            <span class="source-type">arXiv</span>
                            
                        </div>
                        <span class="date">2026-02-12T15:18:46+00:00</span>
                        
                            
                            <a href="http://arxiv.org/abs/2602.12055v1" target="_blank" class="btn-pdf">PDF</a>
                            
                        
                    </div>
                </div>
                
            
        </div>
        
        <footer>
            <p>Áî± <a href="https://github.com/your-repo/auto-researcher">AutoResearcher</a> Ëá™Âä®ÁîüÊàê</p>
        </footer>
    </div>
</body>
</html>